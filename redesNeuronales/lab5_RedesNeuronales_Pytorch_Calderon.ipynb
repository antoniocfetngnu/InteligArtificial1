{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/antoniocfetngnu/InteligArtificial1/blob/main/redesNeuronales/lab5_RedesNeuronales_Pytorch_Calderon.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T2HWIl36XlPi"
      },
      "source": [
        "#Descripcion Dataset\n",
        "\n",
        "Este conjunto de datos resume un conjunto heterogéneo de características sobre artículos publicados por Mashable en un período de dos años. El objetivo es predecir el número de veces que se comparten en redes sociales (popularidad).\n",
        "Adaptacion Salida (Clasificacion)\n",
        "\n",
        "En este conjunto de datos, la columna \"shares\" se considera como la etiqueta (o salida) que se busca predecir. Para facilitar la clasificación, se ha adaptado la columna de \"shares\" a tres categorías de salida: 0, 1 y 2. Estas categorías corresponden a la cantidad de veces que se comparten en redes sociales, dividiendo el valor original de \"shares\" entre la media y luego dividiendo entre 2 para obtener tres categorías distintas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RrkbuyYDXlPk"
      },
      "source": [
        "# utilizado para la manipulación de directorios y rutas\n",
        "import os\n",
        "# Cálculo científico y vectorial para python\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "# Libreria para graficos\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "\n",
        "import torch\n",
        "from torch import optim  # For optimizers like SGD, Adam, etc.\n",
        "from torch import nn  # All neural network modules\n",
        "from torch.utils.data import TensorDataset,DataLoader  # Gives easier dataset managment by creating mini batches etc.\n",
        "\n",
        "import torchvision # torch package for vision related things\n",
        "import torchvision.transforms as transforms  # Transformations we can perform on our dataset for augmentation\n",
        "import torch.nn.functional as F  # Parameterless functions, like (some) activation functions\n",
        "import torchvision.datasets as datasets  # Standard datasets\n",
        "from tqdm import tqdm  # For nice progress bar!\n",
        "\n",
        "from scipy import optimize\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import random\n",
        "\n",
        "# le dice a matplotlib que incruste gráficos en el cuaderno\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Preparación del Dataset\n",
        "Se realizó la misma preparación que se realizó con el ejercicio sin Pytorch, para poder tener una comparacion entre ambos resultados."
      ],
      "metadata": {
        "id": "yerK4CTYEJ81"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('OnlineNewsPopularity.csv')\n",
        "pd.set_option('display.max_columns', None)\n",
        "valor_maximo = df[' shares'].max()\n",
        "valor_minimo = df[' shares'].min()\n",
        "\n",
        "print(f\"Valor máximo: {valor_maximo}\")\n",
        "print(f\"Valor mínimo: {valor_minimo}\")\n",
        "\n",
        "media_shares = df[' shares'].mean()\n",
        "\n",
        "# Divide los valores en 3 categorías basadas en la media\n",
        "df['categoria_shares'] = pd.cut(df[' shares'], bins=[0, media_shares/2, media_shares, float('inf')], labels=[0, 1, 2], right=False)\n",
        "\n",
        "# Verificar los resultados\n",
        "cantidad_por_categoria = df['categoria_shares'].value_counts()\n",
        "print(\"Cantidad por categoría actualizada:\")\n",
        "print(cantidad_por_categoria)\n",
        "\n",
        "\n",
        "df = df.drop('url', axis=1)\n",
        "df = df.dropna()\n",
        "print(\"Categorias:\\n0: Desde 0 a \",media_shares/2,\"\\n1: Desde \",media_shares/2,\" hasta \",media_shares,\"\\n2: Desde \",media_shares, \" en adelante\")\n",
        "df['categoria_shares'] = df['categoria_shares'].astype(int)\n",
        "\n",
        "\n",
        "# Verificar el cambio\n",
        "print(df['categoria_shares'].head(10))\n",
        "\n",
        "df = df.drop(' shares', axis=1)\n",
        "\n",
        "print(df.describe())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fD2S8bjHdRRo",
        "outputId": "1ba77732-72fd-4b02-f8fc-dd14b6cd2b3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Valor máximo: 843300\n",
            "Valor mínimo: 1\n",
            "Cantidad por categoría actualizada:\n",
            "categoria_shares\n",
            "0    22542\n",
            "1     9023\n",
            "2     8079\n",
            "Name: count, dtype: int64\n",
            "Categorias:\n",
            "0: Desde 0 a  1697.6900918171727 \n",
            "1: Desde  1697.6900918171727  hasta  3395.3801836343455 \n",
            "2: Desde  3395.3801836343455  en adelante\n",
            "0    0\n",
            "1    0\n",
            "2    0\n",
            "3    0\n",
            "4    0\n",
            "5    0\n",
            "6    0\n",
            "7    0\n",
            "8    2\n",
            "9    0\n",
            "Name: categoria_shares, dtype: int64\n",
            "          timedelta   n_tokens_title   n_tokens_content   n_unique_tokens  \\\n",
            "count  39644.000000     39644.000000       39644.000000      39644.000000   \n",
            "mean     354.530471        10.398749         546.514731          0.548216   \n",
            "std      214.163767         2.114037         471.107508          3.520708   \n",
            "min        8.000000         2.000000           0.000000          0.000000   \n",
            "25%      164.000000         9.000000         246.000000          0.470870   \n",
            "50%      339.000000        10.000000         409.000000          0.539226   \n",
            "75%      542.000000        12.000000         716.000000          0.608696   \n",
            "max      731.000000        23.000000        8474.000000        701.000000   \n",
            "\n",
            "        n_non_stop_words   n_non_stop_unique_tokens     num_hrefs  \\\n",
            "count       39644.000000               39644.000000  39644.000000   \n",
            "mean            0.996469                   0.689175     10.883690   \n",
            "std             5.231231                   3.264816     11.332017   \n",
            "min             0.000000                   0.000000      0.000000   \n",
            "25%             1.000000                   0.625739      4.000000   \n",
            "50%             1.000000                   0.690476      8.000000   \n",
            "75%             1.000000                   0.754630     14.000000   \n",
            "max          1042.000000                 650.000000    304.000000   \n",
            "\n",
            "        num_self_hrefs      num_imgs    num_videos   average_token_length  \\\n",
            "count     39644.000000  39644.000000  39644.000000           39644.000000   \n",
            "mean          3.293638      4.544143      1.249874               4.548239   \n",
            "std           3.855141      8.309434      4.107855               0.844406   \n",
            "min           0.000000      0.000000      0.000000               0.000000   \n",
            "25%           1.000000      1.000000      0.000000               4.478404   \n",
            "50%           3.000000      1.000000      0.000000               4.664082   \n",
            "75%           4.000000      4.000000      1.000000               4.854839   \n",
            "max         116.000000    128.000000     91.000000               8.041534   \n",
            "\n",
            "        num_keywords   data_channel_is_lifestyle  \\\n",
            "count   39644.000000                39644.000000   \n",
            "mean        7.223767                    0.052946   \n",
            "std         1.909130                    0.223929   \n",
            "min         1.000000                    0.000000   \n",
            "25%         6.000000                    0.000000   \n",
            "50%         7.000000                    0.000000   \n",
            "75%         9.000000                    0.000000   \n",
            "max        10.000000                    1.000000   \n",
            "\n",
            "        data_channel_is_entertainment   data_channel_is_bus  \\\n",
            "count                    39644.000000          39644.000000   \n",
            "mean                         0.178009              0.157855   \n",
            "std                          0.382525              0.364610   \n",
            "min                          0.000000              0.000000   \n",
            "25%                          0.000000              0.000000   \n",
            "50%                          0.000000              0.000000   \n",
            "75%                          0.000000              0.000000   \n",
            "max                          1.000000              1.000000   \n",
            "\n",
            "        data_channel_is_socmed   data_channel_is_tech   data_channel_is_world  \\\n",
            "count             39644.000000           39644.000000            39644.000000   \n",
            "mean                  0.058597               0.185299                0.212567   \n",
            "std                   0.234871               0.388545                0.409129   \n",
            "min                   0.000000               0.000000                0.000000   \n",
            "25%                   0.000000               0.000000                0.000000   \n",
            "50%                   0.000000               0.000000                0.000000   \n",
            "75%                   0.000000               0.000000                0.000000   \n",
            "max                   1.000000               1.000000                1.000000   \n",
            "\n",
            "         kw_min_min     kw_max_min    kw_avg_min     kw_min_max  \\\n",
            "count  39644.000000   39644.000000  39644.000000   39644.000000   \n",
            "mean      26.106801    1153.951682    312.366967   13612.354102   \n",
            "std       69.633215    3857.990877    620.783887   57986.029357   \n",
            "min       -1.000000       0.000000     -1.000000       0.000000   \n",
            "25%       -1.000000     445.000000    141.750000       0.000000   \n",
            "50%       -1.000000     660.000000    235.500000    1400.000000   \n",
            "75%        4.000000    1000.000000    357.000000    7900.000000   \n",
            "max      377.000000  298400.000000  42827.857143  843300.000000   \n",
            "\n",
            "          kw_max_max     kw_avg_max    kw_min_avg     kw_max_avg  \\\n",
            "count   39644.000000   39644.000000  39644.000000   39644.000000   \n",
            "mean   752324.066694  259281.938083   1117.146610    5657.211151   \n",
            "std    214502.129573  135102.247285   1137.456951    6098.871957   \n",
            "min         0.000000       0.000000     -1.000000       0.000000   \n",
            "25%    843300.000000  172846.875000      0.000000    3562.101631   \n",
            "50%    843300.000000  244572.222223   1023.635611    4355.688836   \n",
            "75%    843300.000000  330980.000000   2056.781032    6019.953968   \n",
            "max    843300.000000  843300.000000   3613.039819  298400.000000   \n",
            "\n",
            "         kw_avg_avg   self_reference_min_shares   self_reference_max_shares  \\\n",
            "count  39644.000000                39644.000000                39644.000000   \n",
            "mean    3135.858639                 3998.755396                10329.212662   \n",
            "std     1318.150397                19738.670516                41027.576613   \n",
            "min        0.000000                    0.000000                    0.000000   \n",
            "25%     2382.448566                  639.000000                 1100.000000   \n",
            "50%     2870.074878                 1200.000000                 2800.000000   \n",
            "75%     3600.229564                 2600.000000                 8000.000000   \n",
            "max    43567.659946               843300.000000               843300.000000   \n",
            "\n",
            "        self_reference_avg_sharess   weekday_is_monday   weekday_is_tuesday  \\\n",
            "count                 39644.000000        39644.000000         39644.000000   \n",
            "mean                   6401.697580            0.168020             0.186409   \n",
            "std                   24211.332231            0.373889             0.389441   \n",
            "min                       0.000000            0.000000             0.000000   \n",
            "25%                     981.187500            0.000000             0.000000   \n",
            "50%                    2200.000000            0.000000             0.000000   \n",
            "75%                    5200.000000            0.000000             0.000000   \n",
            "max                  843300.000000            1.000000             1.000000   \n",
            "\n",
            "        weekday_is_wednesday   weekday_is_thursday   weekday_is_friday  \\\n",
            "count           39644.000000          39644.000000        39644.000000   \n",
            "mean                0.187544              0.183306            0.143805   \n",
            "std                 0.390353              0.386922            0.350896   \n",
            "min                 0.000000              0.000000            0.000000   \n",
            "25%                 0.000000              0.000000            0.000000   \n",
            "50%                 0.000000              0.000000            0.000000   \n",
            "75%                 0.000000              0.000000            0.000000   \n",
            "max                 1.000000              1.000000            1.000000   \n",
            "\n",
            "        weekday_is_saturday   weekday_is_sunday    is_weekend        LDA_00  \\\n",
            "count          39644.000000        39644.000000  39644.000000  39644.000000   \n",
            "mean               0.061876            0.069039      0.130915      0.184599   \n",
            "std                0.240933            0.253524      0.337312      0.262975   \n",
            "min                0.000000            0.000000      0.000000      0.000000   \n",
            "25%                0.000000            0.000000      0.000000      0.025051   \n",
            "50%                0.000000            0.000000      0.000000      0.033387   \n",
            "75%                0.000000            0.000000      0.000000      0.240958   \n",
            "max                1.000000            1.000000      1.000000      0.926994   \n",
            "\n",
            "             LDA_01        LDA_02        LDA_03        LDA_04  \\\n",
            "count  39644.000000  39644.000000  39644.000000  39644.000000   \n",
            "mean       0.141256      0.216321      0.223770      0.234029   \n",
            "std        0.219707      0.282145      0.295191      0.289183   \n",
            "min        0.000000      0.000000      0.000000      0.000000   \n",
            "25%        0.025012      0.028571      0.028571      0.028574   \n",
            "50%        0.033345      0.040004      0.040001      0.040727   \n",
            "75%        0.150831      0.334218      0.375763      0.399986   \n",
            "max        0.925947      0.919999      0.926534      0.927191   \n",
            "\n",
            "        global_subjectivity   global_sentiment_polarity  \\\n",
            "count          39644.000000                39644.000000   \n",
            "mean               0.443370                    0.119309   \n",
            "std                0.116685                    0.096931   \n",
            "min                0.000000                   -0.393750   \n",
            "25%                0.396167                    0.057757   \n",
            "50%                0.453457                    0.119117   \n",
            "75%                0.508333                    0.177832   \n",
            "max                1.000000                    0.727841   \n",
            "\n",
            "        global_rate_positive_words   global_rate_negative_words  \\\n",
            "count                 39644.000000                 39644.000000   \n",
            "mean                      0.039625                     0.016612   \n",
            "std                       0.017429                     0.010828   \n",
            "min                       0.000000                     0.000000   \n",
            "25%                       0.028384                     0.009615   \n",
            "50%                       0.039023                     0.015337   \n",
            "75%                       0.050279                     0.021739   \n",
            "max                       0.155488                     0.184932   \n",
            "\n",
            "        rate_positive_words   rate_negative_words   avg_positive_polarity  \\\n",
            "count          39644.000000          39644.000000            39644.000000   \n",
            "mean               0.682150              0.287934                0.353825   \n",
            "std                0.190206              0.156156                0.104542   \n",
            "min                0.000000              0.000000                0.000000   \n",
            "25%                0.600000              0.185185                0.306244   \n",
            "50%                0.710526              0.280000                0.358755   \n",
            "75%                0.800000              0.384615                0.411428   \n",
            "max                1.000000              1.000000                1.000000   \n",
            "\n",
            "        min_positive_polarity   max_positive_polarity   avg_negative_polarity  \\\n",
            "count            39644.000000            39644.000000            39644.000000   \n",
            "mean                 0.095446                0.756728               -0.259524   \n",
            "std                  0.071315                0.247786                0.127726   \n",
            "min                  0.000000                0.000000               -1.000000   \n",
            "25%                  0.050000                0.600000               -0.328383   \n",
            "50%                  0.100000                0.800000               -0.253333   \n",
            "75%                  0.100000                1.000000               -0.186905   \n",
            "max                  1.000000                1.000000                0.000000   \n",
            "\n",
            "        min_negative_polarity   max_negative_polarity   title_subjectivity  \\\n",
            "count            39644.000000            39644.000000         39644.000000   \n",
            "mean                -0.521944               -0.107500             0.282353   \n",
            "std                  0.290290                0.095373             0.324247   \n",
            "min                 -1.000000               -1.000000             0.000000   \n",
            "25%                 -0.700000               -0.125000             0.000000   \n",
            "50%                 -0.500000               -0.100000             0.150000   \n",
            "75%                 -0.300000               -0.050000             0.500000   \n",
            "max                  0.000000                0.000000             1.000000   \n",
            "\n",
            "        title_sentiment_polarity   abs_title_subjectivity  \\\n",
            "count               39644.000000             39644.000000   \n",
            "mean                    0.071425                 0.341843   \n",
            "std                     0.265450                 0.188791   \n",
            "min                    -1.000000                 0.000000   \n",
            "25%                     0.000000                 0.166667   \n",
            "50%                     0.000000                 0.500000   \n",
            "75%                     0.150000                 0.500000   \n",
            "max                     1.000000                 0.500000   \n",
            "\n",
            "        abs_title_sentiment_polarity  categoria_shares  \n",
            "count                   39644.000000      39644.000000  \n",
            "mean                        0.156064          0.635178  \n",
            "std                         0.226294          0.799575  \n",
            "min                         0.000000          0.000000  \n",
            "25%                         0.000000          0.000000  \n",
            "50%                         0.000000          0.000000  \n",
            "75%                         0.250000          1.000000  \n",
            "max                         1.000000          2.000000  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = df.drop(columns=['categoria_shares'])  # Características, se eliminan las columnas de etiquetas\n",
        "y = df['categoria_shares']  # Etiquetas\n",
        "\n",
        "# Verificar las dimensiones de X y y\n",
        "print(\"Dimensiones de X:\", X.shape)\n",
        "print(\"Dimensiones de y:\", y.shape)\n",
        "\n",
        "print(y.value_counts())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-CLyr8EnfINl",
        "outputId": "ba6e39dd-37d9-4ad0-879a-28291d9cda75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dimensiones de X: (39644, 59)\n",
            "Dimensiones de y: (39644,)\n",
            "categoria_shares\n",
            "0    22542\n",
            "1     9023\n",
            "2     8079\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Separacion de entrenamiento y Prueba\n"
      ],
      "metadata": {
        "id": "arDOPOMBESiZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)"
      ],
      "metadata": {
        "id": "2ie1i2FrfLhH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Verificar la proporción de clases en los conjuntos de entrenamiento y prueba\n",
        "print(\"Proporción de clases en y_train:\")\n",
        "print(y_train.value_counts(normalize=True))\n",
        "\n",
        "print(\"\\nProporción de clases en y_test:\")\n",
        "print(y_test.value_counts(normalize=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aknYfuedfPi8",
        "outputId": "28dfa8c3-6617-45aa-fcb2-2ef0467b85f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Proporción de clases en y_train:\n",
            "categoria_shares\n",
            "0    0.568627\n",
            "1    0.227589\n",
            "2    0.203784\n",
            "Name: proportion, dtype: float64\n",
            "\n",
            "Proporción de clases en y_test:\n",
            "categoria_shares\n",
            "0    0.568546\n",
            "1    0.227645\n",
            "2    0.203809\n",
            "Name: proportion, dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hA63y9BgDGi-"
      },
      "source": [
        "class RedNeuronalMLS(nn.Module):\n",
        "    def __init__(self, input_size, num_classes):\n",
        "        super(RedNeuronalMLS, self).__init__()\n",
        "        # Our first linear layer take input_size, in this case 784 nodes to 50\n",
        "        # and our second linear layer takes 50 to the num_classes we have, in\n",
        "        # this case 10.\n",
        "        self.fc1 = nn.Linear(input_size, 50)\n",
        "        self.fc2 = nn.Linear(50, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        x here is the mnist images and we run it through fc1, fc2 that we created above.\n",
        "        we also add a ReLU activation function in between and for that (since it has no parameters)\n",
        "        I recommend using nn.functional (F)\n",
        "        \"\"\"\n",
        "        x = self.fc1(x)\n",
        "        x = F.sigmoid(x)\n",
        "        # x = F.sigmoid(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hiperparámetros y sus valores escogidos tratando de alcanzar una buena precisión"
      ],
      "metadata": {
        "id": "8XoIYtAmEXMp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set device cuda for GPU if it's available otherwise run on the CPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "\n",
        "# Hyperparameters of our neural network which depends on the dataset, and\n",
        "# also just experimenting to see what works well (learning rate for example).\n",
        "input_size = X_train.shape[1]\n",
        "num_classes = len(np.unique(y_train))\n",
        "learning_rate = 0.0001\n",
        "batch_size = 10000\n",
        "num_epochs = 8"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iMpn4HgxfTA4",
        "outputId": "b90b922e-1a53-48be-e17f-3ad79fef6e5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Transformacion de nuestro conjunto de datos (dataframe de pandas) en Tensores PyTorch para poder usarlos en el DataLoader"
      ],
      "metadata": {
        "id": "eRdexXTYHjVh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear Tensores PyTorch para los conjuntos de datos de entrenamiento y prueba\n",
        "X_train_tensor = torch.tensor(X_train.values, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train.values, dtype=torch.long)\n",
        "X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32)\n",
        "y_test_tensor = torch.tensor(y_test.values, dtype=torch.long)\n",
        "\n",
        "# Crear el conjunto de datos de entrenamiento y prueba utilizando TensorDataset\n",
        "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
        "\n",
        "# Crear los DataLoader para el entrenamiento y prueba\n",
        "# batch_size = 64  # Elige el tamaño del lote que desees\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n"
      ],
      "metadata": {
        "id": "kXmq3wxLgRl6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_MtcDzEqEqeO"
      },
      "source": [
        "# Initialize network\n",
        "model = RedNeuronalMLS(input_size=input_size, num_classes=num_classes).to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n4QhA5rv4Td0",
        "outputId": "5ac9ca0a-467f-4c60-a21c-16d2b3fad280"
      },
      "source": [
        "model"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RedNeuronalMLS(\n",
              "  (fc1): Linear(in_features=59, out_features=50, bias=True)\n",
              "  (fc2): Linear(in_features=50, out_features=3, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JOXxAOpEFiCT"
      },
      "source": [
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yNVVsm2uXlPl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb64b8a0-b943-4a8f-d30b-021a92e7e5e1"
      },
      "source": [
        "# Train Network\n",
        "for epoch in range(num_epochs):\n",
        "    for batch_idx, (data, targets) in enumerate(tqdm(train_loader)):\n",
        "    # for batch_idx, (data, targets) in enumerate(train_loader):\n",
        "\n",
        "        # Get data to cuda if possible\n",
        "        data = data.to(device=device)\n",
        "        targets = targets.to(device=device)\n",
        "\n",
        "        # print(data.shape)\n",
        "        # Get to correct shape\n",
        "        data = data.reshape(data.shape[0], -1)\n",
        "        print(data.shape)\n",
        "        # print(\"-\"*30)\n",
        "        # forward\n",
        "        scores = model(data)\n",
        "        loss = criterion(scores, targets)\n",
        "\n",
        "        # backward\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "\n",
        "        # gradient descent or adam step\n",
        "        optimizer.step()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4/4 [00:00<00:00,  9.90it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10000, 59])\n",
            "torch.Size([10000, 59])\n",
            "torch.Size([10000, 59])\n",
            "torch.Size([1715, 59])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4/4 [00:00<00:00, 10.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10000, 59])\n",
            "torch.Size([10000, 59])\n",
            "torch.Size([10000, 59])\n",
            "torch.Size([1715, 59])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/4 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10000, 59])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4/4 [00:00<00:00,  9.89it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10000, 59])\n",
            "torch.Size([10000, 59])\n",
            "torch.Size([1715, 59])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/4 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10000, 59])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4/4 [00:00<00:00,  9.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10000, 59])\n",
            "torch.Size([10000, 59])\n",
            "torch.Size([1715, 59])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/4 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10000, 59])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4/4 [00:00<00:00,  7.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10000, 59])\n",
            "torch.Size([10000, 59])\n",
            "torch.Size([1715, 59])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 25%|██▌       | 1/4 [00:00<00:00,  7.97it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10000, 59])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4/4 [00:00<00:00,  6.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10000, 59])\n",
            "torch.Size([10000, 59])\n",
            "torch.Size([1715, 59])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 25%|██▌       | 1/4 [00:00<00:00,  8.87it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10000, 59])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4/4 [00:00<00:00,  6.94it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10000, 59])\n",
            "torch.Size([10000, 59])\n",
            "torch.Size([1715, 59])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 25%|██▌       | 1/4 [00:00<00:00,  8.64it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10000, 59])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4/4 [00:00<00:00,  6.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10000, 59])\n",
            "torch.Size([10000, 59])\n",
            "torch.Size([1715, 59])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 25%|██▌       | 1/4 [00:00<00:00,  9.27it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10000, 59])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4/4 [00:00<00:00,  9.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10000, 59])\n",
            "torch.Size([10000, 59])\n",
            "torch.Size([1715, 59])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/4 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10000, 59])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4/4 [00:00<00:00, 10.19it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10000, 59])\n",
            "torch.Size([10000, 59])\n",
            "torch.Size([1715, 59])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zfpwc1XSGbYs",
        "outputId": "b4026fce-2c2f-4586-8e7c-e8f6a537900b"
      },
      "source": [
        "# Check accuracy on training & test to see how good our model\n",
        "def check_accuracy(loader, model):\n",
        "    num_correct = 0\n",
        "    num_samples = 0\n",
        "    model.eval()\n",
        "\n",
        "    predicciones = []\n",
        "    with torch.no_grad():\n",
        "        for x, y in loader:\n",
        "            x = x.to(device=device)\n",
        "            y = y.to(device=device)\n",
        "            x = x.reshape(x.shape[0], -1)\n",
        "\n",
        "            scores = model(x)\n",
        "            _, predictions = scores.max(1)\n",
        "            predicciones.append(predictions)\n",
        "\n",
        "            num_correct += (predictions == y).sum()\n",
        "            num_samples += predictions.size(0)\n",
        "\n",
        "    model.train()\n",
        "    return num_correct/num_samples, predicciones\n",
        "\n",
        "p_train, pred_train  = check_accuracy(train_loader, model)\n",
        "p_test, pred_test  = check_accuracy(test_loader, model)\n",
        "\n",
        "print(f\"Accuracy on training set: {p_train*100:.2f}\")\n",
        "print(f\"Accuracy on test set: {p_test*100:.2f}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on training set: 56.82\n",
            "Accuracy on test set: 56.79\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Durante el proceso de experimentación utilizando PyTorch, se exploraron y ajustaron parámetros que influyen en la eficacia y precisión del modelo de red neuronal:\n",
        "\n",
        "**Learning Rate **(Tasa de Aprendizaje): Este parámetro determina la magnitud de los ajustes que se aplican a los pesos del modelo durante cada iteración del entrenamiento.\n",
        "\n",
        "**Número de Epochs **(Épocas): Las épocas representan la cantidad de veces que el modelo ve el conjunto de datos completo durante el entrenamiento.\n",
        "\n",
        "**Batch Size** (Tamaño del Lote): Este parámetro define la cantidad de ejemplos de entrenamiento que se utilizan en cada paso de actualización de los pesos del modelo.\n",
        "\n",
        "En conclusión, se obtuvo una precisíon un poco inferior pero similar al anterior ejercicio realizado sin Pytorch, lo que nos puede dar un indicativo que la precisión alcanzada en el anterior ejercicio no se debe a un problema con la forma en que se desarrolló el ejercicio, sin embargo eso no significa que una precision del 56-58% sea la esperada.\n",
        "\n",
        "La utilización de PyTorch permitió un control detallado y eficaz sobre los parámetros del modelo de red neuronal.\n",
        "Luego de modificar los valores de los parametros buscando una mejor precision, (mas epocas, menor/ tamaño de lotes, un coeficiente de aprendizaje mas pequeño para que tarde mas pero buscando precision)\n",
        "logramos alcanzar una precision de 56% como maximo, y una de 20% como mínimo, por tanto se mantuvo los valores con los que se obtuvo mayor precisión.\n"
      ],
      "metadata": {
        "id": "pUd6poOnGSX1"
      }
    }
  ]
}