{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/antoniocfetngnu/InteligArtificial1/blob/main/redesNeuronales/lab05_RedesNeuronales_Caldeorn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Descripcion Dataset\n",
        "\n",
        "Este conjunto de datos resume un conjunto heterogéneo de características sobre artículos publicados por Mashable en un período de dos años. El objetivo es predecir el número de veces que se comparten en redes sociales (popularidad).\n",
        "\n",
        "###Adaptacion Salida (Clasificacion)\n",
        "En este conjunto de datos, la columna \"shares\" se considera como la etiqueta (o salida) que se busca predecir. Para facilitar la clasificación, se ha adaptado la columna de \"shares\" a tres categorías de salida: 0, 1 y 2. Estas categorías corresponden a la cantidad de veces que se comparten en redes sociales, dividiendo el valor original de \"shares\" entre la media y luego dividiendo entre 2 para obtener tres categorías distintas."
      ],
      "metadata": {
        "id": "xYFD86BG38pc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Importacion de librerias"
      ],
      "metadata": {
        "id": "LvYhjqD94rTH"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "stUHtyXwtUTc"
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from matplotlib import pyplot\n",
        "from scipy import optimize\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import random\n",
        "# tells matplotlib to embed plots within the notebook\n",
        "%matplotlib inline"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Preparación del Dataset\n",
        "###Importacion"
      ],
      "metadata": {
        "id": "42h_LqRc457i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('OnlineNewsPopularity.csv')"
      ],
      "metadata": {
        "id": "x50MUrrfwHQH"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.set_option('display.max_columns', None)\n",
        "print(df.describe())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mk3kjVtfvrWR",
        "outputId": "5ca0e073-34b8-435c-f6a9-5e0a279b985a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "          timedelta   n_tokens_title   n_tokens_content   n_unique_tokens  \\\n",
            "count  39644.000000     39644.000000       39644.000000      39644.000000   \n",
            "mean     354.530471        10.398749         546.514731          0.548216   \n",
            "std      214.163767         2.114037         471.107508          3.520708   \n",
            "min        8.000000         2.000000           0.000000          0.000000   \n",
            "25%      164.000000         9.000000         246.000000          0.470870   \n",
            "50%      339.000000        10.000000         409.000000          0.539226   \n",
            "75%      542.000000        12.000000         716.000000          0.608696   \n",
            "max      731.000000        23.000000        8474.000000        701.000000   \n",
            "\n",
            "        n_non_stop_words   n_non_stop_unique_tokens     num_hrefs  \\\n",
            "count       39644.000000               39644.000000  39644.000000   \n",
            "mean            0.996469                   0.689175     10.883690   \n",
            "std             5.231231                   3.264816     11.332017   \n",
            "min             0.000000                   0.000000      0.000000   \n",
            "25%             1.000000                   0.625739      4.000000   \n",
            "50%             1.000000                   0.690476      8.000000   \n",
            "75%             1.000000                   0.754630     14.000000   \n",
            "max          1042.000000                 650.000000    304.000000   \n",
            "\n",
            "        num_self_hrefs      num_imgs    num_videos   average_token_length  \\\n",
            "count     39644.000000  39644.000000  39644.000000           39644.000000   \n",
            "mean          3.293638      4.544143      1.249874               4.548239   \n",
            "std           3.855141      8.309434      4.107855               0.844406   \n",
            "min           0.000000      0.000000      0.000000               0.000000   \n",
            "25%           1.000000      1.000000      0.000000               4.478404   \n",
            "50%           3.000000      1.000000      0.000000               4.664082   \n",
            "75%           4.000000      4.000000      1.000000               4.854839   \n",
            "max         116.000000    128.000000     91.000000               8.041534   \n",
            "\n",
            "        num_keywords   data_channel_is_lifestyle  \\\n",
            "count   39644.000000                39644.000000   \n",
            "mean        7.223767                    0.052946   \n",
            "std         1.909130                    0.223929   \n",
            "min         1.000000                    0.000000   \n",
            "25%         6.000000                    0.000000   \n",
            "50%         7.000000                    0.000000   \n",
            "75%         9.000000                    0.000000   \n",
            "max        10.000000                    1.000000   \n",
            "\n",
            "        data_channel_is_entertainment   data_channel_is_bus  \\\n",
            "count                    39644.000000          39644.000000   \n",
            "mean                         0.178009              0.157855   \n",
            "std                          0.382525              0.364610   \n",
            "min                          0.000000              0.000000   \n",
            "25%                          0.000000              0.000000   \n",
            "50%                          0.000000              0.000000   \n",
            "75%                          0.000000              0.000000   \n",
            "max                          1.000000              1.000000   \n",
            "\n",
            "        data_channel_is_socmed   data_channel_is_tech   data_channel_is_world  \\\n",
            "count             39644.000000           39644.000000            39644.000000   \n",
            "mean                  0.058597               0.185299                0.212567   \n",
            "std                   0.234871               0.388545                0.409129   \n",
            "min                   0.000000               0.000000                0.000000   \n",
            "25%                   0.000000               0.000000                0.000000   \n",
            "50%                   0.000000               0.000000                0.000000   \n",
            "75%                   0.000000               0.000000                0.000000   \n",
            "max                   1.000000               1.000000                1.000000   \n",
            "\n",
            "         kw_min_min     kw_max_min    kw_avg_min     kw_min_max  \\\n",
            "count  39644.000000   39644.000000  39644.000000   39644.000000   \n",
            "mean      26.106801    1153.951682    312.366967   13612.354102   \n",
            "std       69.633215    3857.990877    620.783887   57986.029357   \n",
            "min       -1.000000       0.000000     -1.000000       0.000000   \n",
            "25%       -1.000000     445.000000    141.750000       0.000000   \n",
            "50%       -1.000000     660.000000    235.500000    1400.000000   \n",
            "75%        4.000000    1000.000000    357.000000    7900.000000   \n",
            "max      377.000000  298400.000000  42827.857143  843300.000000   \n",
            "\n",
            "          kw_max_max     kw_avg_max    kw_min_avg     kw_max_avg  \\\n",
            "count   39644.000000   39644.000000  39644.000000   39644.000000   \n",
            "mean   752324.066694  259281.938083   1117.146610    5657.211151   \n",
            "std    214502.129573  135102.247285   1137.456951    6098.871957   \n",
            "min         0.000000       0.000000     -1.000000       0.000000   \n",
            "25%    843300.000000  172846.875000      0.000000    3562.101631   \n",
            "50%    843300.000000  244572.222223   1023.635611    4355.688836   \n",
            "75%    843300.000000  330980.000000   2056.781032    6019.953968   \n",
            "max    843300.000000  843300.000000   3613.039819  298400.000000   \n",
            "\n",
            "         kw_avg_avg   self_reference_min_shares   self_reference_max_shares  \\\n",
            "count  39644.000000                39644.000000                39644.000000   \n",
            "mean    3135.858639                 3998.755396                10329.212662   \n",
            "std     1318.150397                19738.670516                41027.576613   \n",
            "min        0.000000                    0.000000                    0.000000   \n",
            "25%     2382.448566                  639.000000                 1100.000000   \n",
            "50%     2870.074878                 1200.000000                 2800.000000   \n",
            "75%     3600.229564                 2600.000000                 8000.000000   \n",
            "max    43567.659946               843300.000000               843300.000000   \n",
            "\n",
            "        self_reference_avg_sharess   weekday_is_monday   weekday_is_tuesday  \\\n",
            "count                 39644.000000        39644.000000         39644.000000   \n",
            "mean                   6401.697580            0.168020             0.186409   \n",
            "std                   24211.332231            0.373889             0.389441   \n",
            "min                       0.000000            0.000000             0.000000   \n",
            "25%                     981.187500            0.000000             0.000000   \n",
            "50%                    2200.000000            0.000000             0.000000   \n",
            "75%                    5200.000000            0.000000             0.000000   \n",
            "max                  843300.000000            1.000000             1.000000   \n",
            "\n",
            "        weekday_is_wednesday   weekday_is_thursday   weekday_is_friday  \\\n",
            "count           39644.000000          39644.000000        39644.000000   \n",
            "mean                0.187544              0.183306            0.143805   \n",
            "std                 0.390353              0.386922            0.350896   \n",
            "min                 0.000000              0.000000            0.000000   \n",
            "25%                 0.000000              0.000000            0.000000   \n",
            "50%                 0.000000              0.000000            0.000000   \n",
            "75%                 0.000000              0.000000            0.000000   \n",
            "max                 1.000000              1.000000            1.000000   \n",
            "\n",
            "        weekday_is_saturday   weekday_is_sunday    is_weekend        LDA_00  \\\n",
            "count          39644.000000        39644.000000  39644.000000  39644.000000   \n",
            "mean               0.061876            0.069039      0.130915      0.184599   \n",
            "std                0.240933            0.253524      0.337312      0.262975   \n",
            "min                0.000000            0.000000      0.000000      0.000000   \n",
            "25%                0.000000            0.000000      0.000000      0.025051   \n",
            "50%                0.000000            0.000000      0.000000      0.033387   \n",
            "75%                0.000000            0.000000      0.000000      0.240958   \n",
            "max                1.000000            1.000000      1.000000      0.926994   \n",
            "\n",
            "             LDA_01        LDA_02        LDA_03        LDA_04  \\\n",
            "count  39644.000000  39644.000000  39644.000000  39644.000000   \n",
            "mean       0.141256      0.216321      0.223770      0.234029   \n",
            "std        0.219707      0.282145      0.295191      0.289183   \n",
            "min        0.000000      0.000000      0.000000      0.000000   \n",
            "25%        0.025012      0.028571      0.028571      0.028574   \n",
            "50%        0.033345      0.040004      0.040001      0.040727   \n",
            "75%        0.150831      0.334218      0.375763      0.399986   \n",
            "max        0.925947      0.919999      0.926534      0.927191   \n",
            "\n",
            "        global_subjectivity   global_sentiment_polarity  \\\n",
            "count          39644.000000                39644.000000   \n",
            "mean               0.443370                    0.119309   \n",
            "std                0.116685                    0.096931   \n",
            "min                0.000000                   -0.393750   \n",
            "25%                0.396167                    0.057757   \n",
            "50%                0.453457                    0.119117   \n",
            "75%                0.508333                    0.177832   \n",
            "max                1.000000                    0.727841   \n",
            "\n",
            "        global_rate_positive_words   global_rate_negative_words  \\\n",
            "count                 39644.000000                 39644.000000   \n",
            "mean                      0.039625                     0.016612   \n",
            "std                       0.017429                     0.010828   \n",
            "min                       0.000000                     0.000000   \n",
            "25%                       0.028384                     0.009615   \n",
            "50%                       0.039023                     0.015337   \n",
            "75%                       0.050279                     0.021739   \n",
            "max                       0.155488                     0.184932   \n",
            "\n",
            "        rate_positive_words   rate_negative_words   avg_positive_polarity  \\\n",
            "count          39644.000000          39644.000000            39644.000000   \n",
            "mean               0.682150              0.287934                0.353825   \n",
            "std                0.190206              0.156156                0.104542   \n",
            "min                0.000000              0.000000                0.000000   \n",
            "25%                0.600000              0.185185                0.306244   \n",
            "50%                0.710526              0.280000                0.358755   \n",
            "75%                0.800000              0.384615                0.411428   \n",
            "max                1.000000              1.000000                1.000000   \n",
            "\n",
            "        min_positive_polarity   max_positive_polarity   avg_negative_polarity  \\\n",
            "count            39644.000000            39644.000000            39644.000000   \n",
            "mean                 0.095446                0.756728               -0.259524   \n",
            "std                  0.071315                0.247786                0.127726   \n",
            "min                  0.000000                0.000000               -1.000000   \n",
            "25%                  0.050000                0.600000               -0.328383   \n",
            "50%                  0.100000                0.800000               -0.253333   \n",
            "75%                  0.100000                1.000000               -0.186905   \n",
            "max                  1.000000                1.000000                0.000000   \n",
            "\n",
            "        min_negative_polarity   max_negative_polarity   title_subjectivity  \\\n",
            "count            39644.000000            39644.000000         39644.000000   \n",
            "mean                -0.521944               -0.107500             0.282353   \n",
            "std                  0.290290                0.095373             0.324247   \n",
            "min                 -1.000000               -1.000000             0.000000   \n",
            "25%                 -0.700000               -0.125000             0.000000   \n",
            "50%                 -0.500000               -0.100000             0.150000   \n",
            "75%                 -0.300000               -0.050000             0.500000   \n",
            "max                  0.000000                0.000000             1.000000   \n",
            "\n",
            "        title_sentiment_polarity   abs_title_subjectivity  \\\n",
            "count               39644.000000             39644.000000   \n",
            "mean                    0.071425                 0.341843   \n",
            "std                     0.265450                 0.188791   \n",
            "min                    -1.000000                 0.000000   \n",
            "25%                     0.000000                 0.166667   \n",
            "50%                     0.000000                 0.500000   \n",
            "75%                     0.150000                 0.500000   \n",
            "max                     1.000000                 0.500000   \n",
            "\n",
            "        abs_title_sentiment_polarity         shares  \n",
            "count                   39644.000000   39644.000000  \n",
            "mean                        0.156064    3395.380184  \n",
            "std                         0.226294   11626.950749  \n",
            "min                         0.000000       1.000000  \n",
            "25%                         0.000000     946.000000  \n",
            "50%                         0.000000    1400.000000  \n",
            "75%                         0.250000    2800.000000  \n",
            "max                         1.000000  843300.000000  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N0hbNvj3wiAA",
        "outputId": "b69a7de7-7773-429f-98c1-a63ac9b48e79"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                 url   timedelta  \\\n",
            "0  http://mashable.com/2013/01/07/amazon-instant-...       731.0   \n",
            "1  http://mashable.com/2013/01/07/ap-samsung-spon...       731.0   \n",
            "2  http://mashable.com/2013/01/07/apple-40-billio...       731.0   \n",
            "3  http://mashable.com/2013/01/07/astronaut-notre...       731.0   \n",
            "4   http://mashable.com/2013/01/07/att-u-verse-apps/       731.0   \n",
            "\n",
            "    n_tokens_title   n_tokens_content   n_unique_tokens   n_non_stop_words  \\\n",
            "0             12.0              219.0          0.663594                1.0   \n",
            "1              9.0              255.0          0.604743                1.0   \n",
            "2              9.0              211.0          0.575130                1.0   \n",
            "3              9.0              531.0          0.503788                1.0   \n",
            "4             13.0             1072.0          0.415646                1.0   \n",
            "\n",
            "    n_non_stop_unique_tokens   num_hrefs   num_self_hrefs   num_imgs  \\\n",
            "0                   0.815385         4.0              2.0        1.0   \n",
            "1                   0.791946         3.0              1.0        1.0   \n",
            "2                   0.663866         3.0              1.0        1.0   \n",
            "3                   0.665635         9.0              0.0        1.0   \n",
            "4                   0.540890        19.0             19.0       20.0   \n",
            "\n",
            "    num_videos   average_token_length   num_keywords  \\\n",
            "0          0.0               4.680365            5.0   \n",
            "1          0.0               4.913725            4.0   \n",
            "2          0.0               4.393365            6.0   \n",
            "3          0.0               4.404896            7.0   \n",
            "4          0.0               4.682836            7.0   \n",
            "\n",
            "    data_channel_is_lifestyle   data_channel_is_entertainment  \\\n",
            "0                         0.0                             1.0   \n",
            "1                         0.0                             0.0   \n",
            "2                         0.0                             0.0   \n",
            "3                         0.0                             1.0   \n",
            "4                         0.0                             0.0   \n",
            "\n",
            "    data_channel_is_bus   data_channel_is_socmed   data_channel_is_tech  \\\n",
            "0                   0.0                      0.0                    0.0   \n",
            "1                   1.0                      0.0                    0.0   \n",
            "2                   1.0                      0.0                    0.0   \n",
            "3                   0.0                      0.0                    0.0   \n",
            "4                   0.0                      0.0                    1.0   \n",
            "\n",
            "    data_channel_is_world   kw_min_min   kw_max_min   kw_avg_min   kw_min_max  \\\n",
            "0                     0.0          0.0          0.0          0.0          0.0   \n",
            "1                     0.0          0.0          0.0          0.0          0.0   \n",
            "2                     0.0          0.0          0.0          0.0          0.0   \n",
            "3                     0.0          0.0          0.0          0.0          0.0   \n",
            "4                     0.0          0.0          0.0          0.0          0.0   \n",
            "\n",
            "    kw_max_max   kw_avg_max   kw_min_avg   kw_max_avg   kw_avg_avg  \\\n",
            "0          0.0          0.0          0.0          0.0          0.0   \n",
            "1          0.0          0.0          0.0          0.0          0.0   \n",
            "2          0.0          0.0          0.0          0.0          0.0   \n",
            "3          0.0          0.0          0.0          0.0          0.0   \n",
            "4          0.0          0.0          0.0          0.0          0.0   \n",
            "\n",
            "    self_reference_min_shares   self_reference_max_shares  \\\n",
            "0                       496.0                       496.0   \n",
            "1                         0.0                         0.0   \n",
            "2                       918.0                       918.0   \n",
            "3                         0.0                         0.0   \n",
            "4                       545.0                     16000.0   \n",
            "\n",
            "    self_reference_avg_sharess   weekday_is_monday   weekday_is_tuesday  \\\n",
            "0                   496.000000                 1.0                  0.0   \n",
            "1                     0.000000                 1.0                  0.0   \n",
            "2                   918.000000                 1.0                  0.0   \n",
            "3                     0.000000                 1.0                  0.0   \n",
            "4                  3151.157895                 1.0                  0.0   \n",
            "\n",
            "    weekday_is_wednesday   weekday_is_thursday   weekday_is_friday  \\\n",
            "0                    0.0                   0.0                 0.0   \n",
            "1                    0.0                   0.0                 0.0   \n",
            "2                    0.0                   0.0                 0.0   \n",
            "3                    0.0                   0.0                 0.0   \n",
            "4                    0.0                   0.0                 0.0   \n",
            "\n",
            "    weekday_is_saturday   weekday_is_sunday   is_weekend    LDA_00    LDA_01  \\\n",
            "0                   0.0                 0.0          0.0  0.500331  0.378279   \n",
            "1                   0.0                 0.0          0.0  0.799756  0.050047   \n",
            "2                   0.0                 0.0          0.0  0.217792  0.033334   \n",
            "3                   0.0                 0.0          0.0  0.028573  0.419300   \n",
            "4                   0.0                 0.0          0.0  0.028633  0.028794   \n",
            "\n",
            "     LDA_02    LDA_03    LDA_04   global_subjectivity  \\\n",
            "0  0.040005  0.041263  0.040123              0.521617   \n",
            "1  0.050096  0.050101  0.050001              0.341246   \n",
            "2  0.033351  0.033334  0.682188              0.702222   \n",
            "3  0.494651  0.028905  0.028572              0.429850   \n",
            "4  0.028575  0.028572  0.885427              0.513502   \n",
            "\n",
            "    global_sentiment_polarity   global_rate_positive_words  \\\n",
            "0                    0.092562                     0.045662   \n",
            "1                    0.148948                     0.043137   \n",
            "2                    0.323333                     0.056872   \n",
            "3                    0.100705                     0.041431   \n",
            "4                    0.281003                     0.074627   \n",
            "\n",
            "    global_rate_negative_words   rate_positive_words   rate_negative_words  \\\n",
            "0                     0.013699              0.769231              0.230769   \n",
            "1                     0.015686              0.733333              0.266667   \n",
            "2                     0.009479              0.857143              0.142857   \n",
            "3                     0.020716              0.666667              0.333333   \n",
            "4                     0.012127              0.860215              0.139785   \n",
            "\n",
            "    avg_positive_polarity   min_positive_polarity   max_positive_polarity  \\\n",
            "0                0.378636                0.100000                     0.7   \n",
            "1                0.286915                0.033333                     0.7   \n",
            "2                0.495833                0.100000                     1.0   \n",
            "3                0.385965                0.136364                     0.8   \n",
            "4                0.411127                0.033333                     1.0   \n",
            "\n",
            "    avg_negative_polarity   min_negative_polarity   max_negative_polarity  \\\n",
            "0               -0.350000                  -0.600               -0.200000   \n",
            "1               -0.118750                  -0.125               -0.100000   \n",
            "2               -0.466667                  -0.800               -0.133333   \n",
            "3               -0.369697                  -0.600               -0.166667   \n",
            "4               -0.220192                  -0.500               -0.050000   \n",
            "\n",
            "    title_subjectivity   title_sentiment_polarity   abs_title_subjectivity  \\\n",
            "0             0.500000                  -0.187500                 0.000000   \n",
            "1             0.000000                   0.000000                 0.500000   \n",
            "2             0.000000                   0.000000                 0.500000   \n",
            "3             0.000000                   0.000000                 0.500000   \n",
            "4             0.454545                   0.136364                 0.045455   \n",
            "\n",
            "    abs_title_sentiment_polarity   shares  \n",
            "0                       0.187500      593  \n",
            "1                       0.000000      711  \n",
            "2                       0.000000     1500  \n",
            "3                       0.000000     1200  \n",
            "4                       0.136364      505  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Verifiacion del valor maximo y minimo en la columna Shares para hacer una correcta adaptación"
      ],
      "metadata": {
        "id": "PJ1l1Fsn5IQt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "valor_maximo = df[' shares'].max()\n",
        "valor_minimo = df[' shares'].min()\n",
        "\n",
        "print(f\"Valor máximo: {valor_maximo}\")\n",
        "print(f\"Valor mínimo: {valor_minimo}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-EvscQv12NvP",
        "outputId": "ef967726-5cd1-4a1b-c9c5-f862cd66633a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Valor máximo: 843300\n",
            "Valor mínimo: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adaptación de la salida en 3 valores de clasificacion 0,1,2.\n",
        "Los \"Shares\" indican la popularidad de un artículo publicado según la cantidad de compartidos. Por tanto en la adaptacion se tomará 3 categorías\n",
        "No Popular=0, Neutro=1, Popular=2"
      ],
      "metadata": {
        "id": "sJzjqS705O67"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "media_shares = df[' shares'].mean()\n",
        "\n",
        "# Divide los valores en 3 categorías basadas en la media\n",
        "df['categoria_shares'] = pd.cut(df[' shares'], bins=[0, media_shares/2, media_shares, float('inf')], labels=[0, 1, 2], right=False)\n",
        "\n",
        "# Verificar los resultados\n",
        "cantidad_por_categoria = df['categoria_shares'].value_counts()\n",
        "print(\"Cantidad por categoría actualizada:\")\n",
        "print(cantidad_por_categoria)\n",
        "print(\"Categorias:\\n0: Desde 0 a \",media_shares/2,\"\\n1: Desde \",media_shares/2,\" hasta \",media_shares,\"\\n2: Desde \",media_shares, \" en adelante\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "54SEHHQf3Q8i",
        "outputId": "c49b48d7-e07a-41c3-d265-89dd5103298b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cantidad por categoría actualizada:\n",
            "categoria_shares\n",
            "0    22542\n",
            "1     9023\n",
            "2     8079\n",
            "Name: count, dtype: int64\n",
            "Categorias:\n",
            "0: Desde 0 a  1697.6900918171727 \n",
            "1: Desde  1697.6900918171727  hasta  3395.3801836343455 \n",
            "2: Desde  3395.3801836343455  en adelante\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comprobacion de tipos de datos mixtos en una misma columna"
      ],
      "metadata": {
        "id": "26-Itg1t5sZR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Obtiene información sobre los tipos de datos de cada columna\n",
        "column_types = df.dtypes\n",
        "\n",
        "# Inicializa una lista para almacenar las columnas con tipos de datos mixtos\n",
        "mixed_type_columns = []\n",
        "\n",
        "# Itera sobre las columnas y verifica si cada una tiene más de un tipo de dato\n",
        "for column in column_types.index:\n",
        "    unique_types = df[column].apply(type).unique()\n",
        "    if len(unique_types) > 1:\n",
        "        mixed_type_columns.append(column)\n",
        "\n",
        "# Imprime las columnas con tipos de datos mixtos\n",
        "print(\"Columnas con tipos de datos mixtos:\")\n",
        "print(mixed_type_columns)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wBjJldGOwsTD",
        "outputId": "cd13899e-0df0-4167-df4b-4e6ff617ab5e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Columnas con tipos de datos mixtos:\n",
            "[]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comprobacion de presencia de valores nulos (en este caso no hay nulos)"
      ],
      "metadata": {
        "id": "B4ONIJlt5xXH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Contar los valores nulos en cada columna del DataFrame\n",
        "valores_nulos_por_columna = df.isna().sum()\n",
        "pd.set_option('display.max_rows', None)\n",
        "pd.set_option('display.max_columns', None)\n",
        "# Mostrar el resultado\n",
        "print(\"Valores nulos por columna:\")\n",
        "print(valores_nulos_por_columna)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ybA4T2Gw5EMh",
        "outputId": "a9ed9551-bc6c-4c6b-ef7b-307249328588"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Valores nulos por columna:\n",
            "url                               0\n",
            " timedelta                        0\n",
            " n_tokens_title                   0\n",
            " n_tokens_content                 0\n",
            " n_unique_tokens                  0\n",
            " n_non_stop_words                 0\n",
            " n_non_stop_unique_tokens         0\n",
            " num_hrefs                        0\n",
            " num_self_hrefs                   0\n",
            " num_imgs                         0\n",
            " num_videos                       0\n",
            " average_token_length             0\n",
            " num_keywords                     0\n",
            " data_channel_is_lifestyle        0\n",
            " data_channel_is_entertainment    0\n",
            " data_channel_is_bus              0\n",
            " data_channel_is_socmed           0\n",
            " data_channel_is_tech             0\n",
            " data_channel_is_world            0\n",
            " kw_min_min                       0\n",
            " kw_max_min                       0\n",
            " kw_avg_min                       0\n",
            " kw_min_max                       0\n",
            " kw_max_max                       0\n",
            " kw_avg_max                       0\n",
            " kw_min_avg                       0\n",
            " kw_max_avg                       0\n",
            " kw_avg_avg                       0\n",
            " self_reference_min_shares        0\n",
            " self_reference_max_shares        0\n",
            " self_reference_avg_sharess       0\n",
            " weekday_is_monday                0\n",
            " weekday_is_tuesday               0\n",
            " weekday_is_wednesday             0\n",
            " weekday_is_thursday              0\n",
            " weekday_is_friday                0\n",
            " weekday_is_saturday              0\n",
            " weekday_is_sunday                0\n",
            " is_weekend                       0\n",
            " LDA_00                           0\n",
            " LDA_01                           0\n",
            " LDA_02                           0\n",
            " LDA_03                           0\n",
            " LDA_04                           0\n",
            " global_subjectivity              0\n",
            " global_sentiment_polarity        0\n",
            " global_rate_positive_words       0\n",
            " global_rate_negative_words       0\n",
            " rate_positive_words              0\n",
            " rate_negative_words              0\n",
            " avg_positive_polarity            0\n",
            " min_positive_polarity            0\n",
            " max_positive_polarity            0\n",
            " avg_negative_polarity            0\n",
            " min_negative_polarity            0\n",
            " max_negative_polarity            0\n",
            " title_subjectivity               0\n",
            " title_sentiment_polarity         0\n",
            " abs_title_subjectivity           0\n",
            " abs_title_sentiment_polarity     0\n",
            " shares                           0\n",
            "categoria_shares                  0\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Eliminacion de columnas innecesarias"
      ],
      "metadata": {
        "id": "knTE1yi0512O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.drop('url', axis=1)"
      ],
      "metadata": {
        "id": "TA-kf93AxFA1"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.dropna()"
      ],
      "metadata": {
        "id": "6DfVlXB_5nK1"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Restablecer las opciones de visualización a sus valores por defecto\n",
        "pd.reset_option('display.max_rows')\n",
        "pd.reset_option('display.max_columns')"
      ],
      "metadata": {
        "id": "gYoB_bMl5iPx"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['categoria_shares'] = df['categoria_shares'].astype(int)\n",
        "\n",
        "# Verificar el cambio\n",
        "print(df['categoria_shares'].head(10))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BVfVY2aE6TXB",
        "outputId": "2fcef724-e1cb-471b-92fd-4e6ce9cdcd35"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0    0\n",
            "1    0\n",
            "2    0\n",
            "3    0\n",
            "4    0\n",
            "5    0\n",
            "6    0\n",
            "7    0\n",
            "8    2\n",
            "9    0\n",
            "Name: categoria_shares, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.drop(' shares', axis=1)"
      ],
      "metadata": {
        "id": "ZDpP628h6XiL"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.describe())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KrMD6v1f6Fdd",
        "outputId": "66392673-d6d1-4453-9364-db2ad7188705"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "          timedelta   n_tokens_title   n_tokens_content   n_unique_tokens  \\\n",
            "count  39644.000000     39644.000000       39644.000000      39644.000000   \n",
            "mean     354.530471        10.398749         546.514731          0.548216   \n",
            "std      214.163767         2.114037         471.107508          3.520708   \n",
            "min        8.000000         2.000000           0.000000          0.000000   \n",
            "25%      164.000000         9.000000         246.000000          0.470870   \n",
            "50%      339.000000        10.000000         409.000000          0.539226   \n",
            "75%      542.000000        12.000000         716.000000          0.608696   \n",
            "max      731.000000        23.000000        8474.000000        701.000000   \n",
            "\n",
            "        n_non_stop_words   n_non_stop_unique_tokens     num_hrefs  \\\n",
            "count       39644.000000               39644.000000  39644.000000   \n",
            "mean            0.996469                   0.689175     10.883690   \n",
            "std             5.231231                   3.264816     11.332017   \n",
            "min             0.000000                   0.000000      0.000000   \n",
            "25%             1.000000                   0.625739      4.000000   \n",
            "50%             1.000000                   0.690476      8.000000   \n",
            "75%             1.000000                   0.754630     14.000000   \n",
            "max          1042.000000                 650.000000    304.000000   \n",
            "\n",
            "        num_self_hrefs      num_imgs    num_videos  ...  \\\n",
            "count     39644.000000  39644.000000  39644.000000  ...   \n",
            "mean          3.293638      4.544143      1.249874  ...   \n",
            "std           3.855141      8.309434      4.107855  ...   \n",
            "min           0.000000      0.000000      0.000000  ...   \n",
            "25%           1.000000      1.000000      0.000000  ...   \n",
            "50%           3.000000      1.000000      0.000000  ...   \n",
            "75%           4.000000      4.000000      1.000000  ...   \n",
            "max         116.000000    128.000000     91.000000  ...   \n",
            "\n",
            "        min_positive_polarity   max_positive_polarity   avg_negative_polarity  \\\n",
            "count            39644.000000            39644.000000            39644.000000   \n",
            "mean                 0.095446                0.756728               -0.259524   \n",
            "std                  0.071315                0.247786                0.127726   \n",
            "min                  0.000000                0.000000               -1.000000   \n",
            "25%                  0.050000                0.600000               -0.328383   \n",
            "50%                  0.100000                0.800000               -0.253333   \n",
            "75%                  0.100000                1.000000               -0.186905   \n",
            "max                  1.000000                1.000000                0.000000   \n",
            "\n",
            "        min_negative_polarity   max_negative_polarity   title_subjectivity  \\\n",
            "count            39644.000000            39644.000000         39644.000000   \n",
            "mean                -0.521944               -0.107500             0.282353   \n",
            "std                  0.290290                0.095373             0.324247   \n",
            "min                 -1.000000               -1.000000             0.000000   \n",
            "25%                 -0.700000               -0.125000             0.000000   \n",
            "50%                 -0.500000               -0.100000             0.150000   \n",
            "75%                 -0.300000               -0.050000             0.500000   \n",
            "max                  0.000000                0.000000             1.000000   \n",
            "\n",
            "        title_sentiment_polarity   abs_title_subjectivity  \\\n",
            "count               39644.000000             39644.000000   \n",
            "mean                    0.071425                 0.341843   \n",
            "std                     0.265450                 0.188791   \n",
            "min                    -1.000000                 0.000000   \n",
            "25%                     0.000000                 0.166667   \n",
            "50%                     0.000000                 0.500000   \n",
            "75%                     0.150000                 0.500000   \n",
            "max                     1.000000                 0.500000   \n",
            "\n",
            "        abs_title_sentiment_polarity  categoria_shares  \n",
            "count                   39644.000000      39644.000000  \n",
            "mean                        0.156064          0.635178  \n",
            "std                         0.226294          0.799575  \n",
            "min                         0.000000          0.000000  \n",
            "25%                         0.000000          0.000000  \n",
            "50%                         0.000000          0.000000  \n",
            "75%                         0.250000          1.000000  \n",
            "max                         1.000000          2.000000  \n",
            "\n",
            "[8 rows x 60 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Separación en entrenamiento y prueba\n"
      ],
      "metadata": {
        "id": "SCjWNHDP6Ct5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = df.drop(columns=['categoria_shares'])  # Características, se eliminan las columnas de etiquetas\n",
        "y = df['categoria_shares']  # Etiquetas\n",
        "\n",
        "# Verificar las dimensiones de X y y\n",
        "print(\"Dimensiones de X:\", X.shape)\n",
        "print(\"Dimensiones de y:\", y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LVwxATKH7KGv",
        "outputId": "003d709e-f137-40db-d0d6-d67e9376eb7a"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dimensiones de X: (39644, 59)\n",
            "Dimensiones de y: (39644,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(y.value_counts())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XYLjfrnk7Q6f",
        "outputId": "3517c683-03e5-425f-800a-c89a3b650441"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "categoria_shares\n",
            "0    22542\n",
            "1     9023\n",
            "2     8079\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)"
      ],
      "metadata": {
        "id": "Epn55I1v7fgz"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Verificar la proporción de clases en los conjuntos de entrenamiento y prueba\n",
        "print(\"Proporción de clases en y_train:\")\n",
        "print(y_train.value_counts(normalize=True))\n",
        "\n",
        "print(\"\\nProporción de clases en y_test:\")\n",
        "print(y_test.value_counts(normalize=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iPyolyGq7rJB",
        "outputId": "5d05aa25-5afa-4cfb-8584-04612f0eb585"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Proporción de clases en y_train:\n",
            "categoria_shares\n",
            "0    0.568627\n",
            "1    0.227589\n",
            "2    0.203784\n",
            "Name: proportion, dtype: float64\n",
            "\n",
            "Proporción de clases en y_test:\n",
            "categoria_shares\n",
            "0    0.568546\n",
            "1    0.227645\n",
            "2    0.203809\n",
            "Name: proportion, dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "nDYVZCVx6JOK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Ajustar input_layer_size al número de características en X_train\n",
        "input_layer_size = X_train.shape[1]\n",
        "\n",
        "# 2. Ajustar num_labels al número de categorías en y_train\n",
        "num_labels = len(np.unique(y_train))"
      ],
      "metadata": {
        "id": "81QnM2ab-Cl2"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Normalizacion"
      ],
      "metadata": {
        "id": "tFjuJX9z6Hxi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalizar características solo en los conjuntos de entrenamiento y prueba de X (caracteristicas)\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X_train)\n",
        "Xp = scaler.transform(X_test)\n",
        "print(X)\n",
        "print('-'*30)\n",
        "print(Xp)\n",
        "m, n = X.shape\n",
        "print('valor de m:',m)\n",
        "print('valor de n:',n)\n",
        "\n",
        "y = y_train.values\n",
        "yp = y_test.values\n",
        "# Verificar las dimensiones de X_train_np y y_train_np\n",
        "print(\"Dimensiones de X_train_np:\", X.shape)\n",
        "print(\"Dimensiones de y_train_np:\", y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3LOX95Q7C0HZ",
        "outputId": "a1959629-282e-4380-aa64-d2693f72949a"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 0.57755307  0.75974389 -0.30732414 ...  1.23685855 -0.22459879\n",
            "   1.07553011]\n",
            " [-1.35468875  0.75974389  0.48013696 ... -0.26696477  0.83678\n",
            "  -0.68838021]\n",
            " [ 0.31555418  0.75974389  1.57079164 ... -0.26696477  0.83678\n",
            "  -0.68838021]\n",
            " ...\n",
            " [-1.08801131 -1.13718598 -0.86317903 ... -0.26696477  0.83678\n",
            "  -0.68838021]\n",
            " [-0.05872995 -1.61141845 -0.83159636 ... -0.26696477  0.83678\n",
            "  -0.68838021]\n",
            " [-1.2049751  -1.61141845 -0.79580268 ... -0.26696477 -0.93218465\n",
            "  -0.68838021]]\n",
            "------------------------------\n",
            "[[ 1.27933582 -0.66295351 -0.51366421 ...  2.55270396 -0.48994349\n",
            "   2.61895164]\n",
            " [ 0.79276645  0.75974389 -0.07571793 ...  0.26563932 -0.22459879\n",
            "  -0.06366198]\n",
            " [-1.27047482  2.65667376 -0.69684366 ...  0.10899106  0.83678\n",
            "  -0.24740263]\n",
            " ...\n",
            " [ 0.9518372  -0.18872105 -0.58104056 ...  0.48494689 -0.22459879\n",
            "   0.19357495]\n",
            " [-1.33597454 -1.13718598 -0.68631611 ... -0.26696477  0.83678\n",
            "  -0.68838021]\n",
            " [-1.34533165  0.75974389  1.11179025 ... -0.26696477  0.83678\n",
            "  -0.68838021]]\n",
            "valor de m: 31715\n",
            "valor de n: 59\n",
            "Dimensiones de X_train_np: (31715, 59)\n",
            "Dimensiones de y_train_np: (31715,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Input Layer Size:\",input_layer_size)\n",
        "print(\"num Labels:\",num_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lykilo-9-Ezd",
        "outputId": "c00673f7-491e-4b87-eadb-b1ca2bf307bb"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input Layer Size: 59\n",
            "num Labels: 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hidden_layer_size = 12\n",
        "# Carga los pesos en las variables Theta1 y Theta2\n",
        "pesos = {}\n",
        "pesos['Theta1'] = np.random.rand(hidden_layer_size, input_layer_size + 1)\n",
        "pesos['Theta2'] = np.random.rand(num_labels, hidden_layer_size + 1)\n",
        "\n",
        "Theta1, Theta2 = pesos['Theta1'], pesos['Theta2']\n",
        "\n",
        "# Desenrollar parámetros\n",
        "print(Theta1.ravel().shape)\n",
        "print(Theta2.ravel().shape)\n",
        "\n",
        "nn_params = np.concatenate([Theta1.ravel(), Theta2.ravel()])\n",
        "print(nn_params.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "veIIUXaBAe_E",
        "outputId": "be796d00-7272-41fc-dac5-30a470d43d4b"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(720,)\n",
            "(39,)\n",
            "(759,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se definen funciones como sigmoid y sigmoidGradient para calcular la función sigmoide y su gradiente"
      ],
      "metadata": {
        "id": "dMxDoIJn-HvI"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IYkAveQh0e87"
      },
      "source": [
        "def sigmoid(z):\n",
        "    \"\"\"\n",
        "    Computes the sigmoid of z.\n",
        "    \"\"\"\n",
        "    return 1.0 / (1.0 + np.exp(-z))\n",
        "\n",
        "\n",
        "def sigmoidGradient(z):\n",
        "\n",
        "    g = np.zeros(z.shape)\n",
        "\n",
        "    g = sigmoid(z) * (1 - sigmoid(z))\n",
        "\n",
        "    return g"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aevzq-rt0vKn"
      },
      "source": [
        "def nnCostFunction(nn_params,\n",
        "                   input_layer_size,\n",
        "                   hidden_layer_size,\n",
        "                   num_labels,\n",
        "                   X, y, lambda_= 0.0):\n",
        "\n",
        "   # Reshape nn_params back into the parameters Theta1 and Theta2, the weight matrices\n",
        "    # for our 2 layer neural network\n",
        "    Theta1 = np.reshape(nn_params[:hidden_layer_size * (input_layer_size + 1)],\n",
        "                        (hidden_layer_size, (input_layer_size + 1)))\n",
        "\n",
        "    Theta2 = np.reshape(nn_params[(hidden_layer_size * (input_layer_size + 1)):],\n",
        "                        (num_labels, (hidden_layer_size + 1)))\n",
        "\n",
        "    m = y.size  # Número de ejemplos de entrenamiento\n",
        "\n",
        "    J = 0  # Inicializar el costo\n",
        "    Theta1_grad = np.zeros(Theta1.shape)  # Inicializar el gradiente de Theta1\n",
        "    Theta2_grad = np.zeros(Theta2.shape)  # Inicializar el gradiente de Theta2\n",
        "\n",
        "    # Añadir una columna de unos a X (a1 es la entrada a la red)\n",
        "    a1 = np.concatenate([np.ones((m, 1)), X], axis=1)\n",
        "\n",
        "    # ----- Comienza el Forward Propagation -----\n",
        "\n",
        "    # Calcular las activaciones de la capa oculta (a2)\n",
        "    z2 = a1.dot(Theta1.T)\n",
        "    a2 = sigmoid(z2)\n",
        "    a2 = np.concatenate([np.ones((a2.shape[0], 1)), a2], axis=1)  # Añadir una columna de unos a a2\n",
        "\n",
        "    # Calcular las activaciones de la capa de salida (a3)\n",
        "    z3 = a2.dot(Theta2.T)\n",
        "    a3 = sigmoid(z3)\n",
        "\n",
        "    # Convertir y en una matriz de etiquetas binarias (one-hot encoding)\n",
        "    y_matrix = np.eye(num_labels)[y]\n",
        "\n",
        "    # Calcular el término de regularización (excluyendo el sesgo de la primera columna)\n",
        "    reg_term = (lambda_ / (2 * m)) * (np.sum(np.square(Theta1[:, 1:])) + np.sum(np.square(Theta2[:, 1:])))\n",
        "\n",
        "    # Calcular la función de costo (J) incluyendo el término de regularización\n",
        "    J = (-1 / m) * np.sum((np.log(a3) * y_matrix) + np.log(1 - a3) * (1 - y_matrix)) + reg_term\n",
        "\n",
        "    # ----- Termina el Forward Propagation -----\n",
        "\n",
        "    # ----- Comienza el Backward Propagation -----\n",
        "\n",
        "    # Calcular el error en la capa de salida (delta_3)\n",
        "    delta_3 = a3 - y_matrix\n",
        "\n",
        "    # Calcular el error en la capa oculta (delta_2) utilizando el gradiente de la función sigmoidea\n",
        "    delta_2 = delta_3.dot(Theta2[:, 1:]) * sigmoidGradient(z2)\n",
        "\n",
        "    # Acumular los gradientes\n",
        "    Delta1 = delta_2.T.dot(a1)\n",
        "    Delta2 = delta_3.T.dot(a2)\n",
        "\n",
        "    # Calcular el gradiente regularizado\n",
        "    Theta1_grad = (1 / m) * Delta1\n",
        "    Theta1_grad[:, 1:] += (lambda_ / m) * Theta1[:, 1:]  # Regularizar Theta1 (excluyendo el sesgo)\n",
        "    Theta2_grad = (1 / m) * Delta2\n",
        "    Theta2_grad[:, 1:] += (lambda_ / m) * Theta2[:, 1:]  # Regularizar Theta2 (excluyendo el sesgo)\n",
        "\n",
        "    # ----- Termina el Backward Propagation -----\n",
        "\n",
        "    # Convertir los gradientes en un único vector\n",
        "    grad = np.concatenate([Theta1_grad.ravel(), Theta2_grad.ravel()])\n",
        "\n",
        "    return J, grad"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k4TjzE2h3BqL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9f56183-7cec-4a02-a4e8-b3152e97d574"
      },
      "source": [
        "lambda_ = 500\n",
        "J, _ = nnCostFunction(nn_params, input_layer_size, hidden_layer_size, num_labels, X, y, lambda_)\n",
        "print('Costo en parametros: %.6f ' % J)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Costo en parametros: 9.481483 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EnKgJRZq-x3U"
      },
      "source": [
        "def randInitializeWeights(L_in, L_out, epsilon_init=0.12):\n",
        "    \"\"\"\n",
        "    Randomly initialize the weights of a layer in a neural network.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    L_in : int\n",
        "        Number of incomming connections.\n",
        "\n",
        "    L_out : int\n",
        "        Number of outgoing connections.\n",
        "\n",
        "    epsilon_init : float, optional\n",
        "        Range of values which the weight can take from a uniform\n",
        "        distribution.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    W : array_like\n",
        "        The weight initialiatized to random values.  Note that W should\n",
        "        be set to a matrix of size(L_out, 1 + L_in) as\n",
        "        the first column of W handles the \"bias\" terms.\"\"\"\n",
        "\n",
        "\n",
        "    W = np.zeros((L_out, 1 + L_in))\n",
        "    W = np.random.rand(L_out, 1 + L_in) * 2 * epsilon_init - epsilon_init\n",
        "\n",
        "    return W"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "znk_8rO0-6fE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ac13346-9d03-4434-b5f0-d4a00e3a60ea"
      },
      "source": [
        "print('Inicialización de parámetros de redes neuronales...')\n",
        "\n",
        "initial_Theta1 = randInitializeWeights(input_layer_size, hidden_layer_size)\n",
        "initial_Theta2 = randInitializeWeights(hidden_layer_size, num_labels)\n",
        "\n",
        "# Desenrrollr parametros\n",
        "initial_nn_params = np.concatenate([initial_Theta1.ravel(), initial_Theta2.ravel()], axis=0)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inicialización de parámetros de redes neuronales...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ysYL_hX_D0k"
      },
      "source": [
        "# After you have completed the assignment, change the maxiter to a larger value to see how more training helps.\n",
        "options = {'maxiter': 10000}\n",
        "\n",
        "# You should also try different values of lambda\n",
        "lambda_ = 200\n",
        "\n",
        "# Create \"short hand\" for the cost function to be minimized\n",
        "costFunction = lambda p: nnCostFunction(p, input_layer_size,\n",
        "                                        hidden_layer_size,\n",
        "                                        num_labels, X, y, lambda_)\n",
        "\n",
        "# Now, costFunction is a function that takes in only one argument (the neural network parameters)\n",
        "res = optimize.minimize(costFunction,\n",
        "                        initial_nn_params,\n",
        "                        jac=True,\n",
        "                        method='L-BFGS-B',  # Cambio a otro método de optimización\n",
        "                        options=options)\n",
        "\n",
        "# Get the solution of the optimization\n",
        "nn_params = res.x\n",
        "\n",
        "# Obtain Theta1 and Theta2 back from nn_params\n",
        "Theta1 = np.reshape(nn_params[:hidden_layer_size * (input_layer_size + 1)],\n",
        "                    (hidden_layer_size, (input_layer_size + 1)))\n",
        "\n",
        "Theta2 = np.reshape(nn_params[(hidden_layer_size * (input_layer_size + 1)):],\n",
        "                    (num_labels, (hidden_layer_size + 1)))"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XDnQrQM4_0Ct"
      },
      "source": [
        "def predict(Theta1, Theta2, X):\n",
        "    \"\"\"\n",
        "    Predict the label of an input given a trained neural network\n",
        "    Outputs the predicted label of X given the trained weights of a neural\n",
        "    network(Theta1, Theta2)\n",
        "    \"\"\"\n",
        "    # Useful values\n",
        "    m = X.shape[0]\n",
        "    num_labels = Theta2.shape[0]\n",
        "\n",
        "    # You need to return the following variables correctly\n",
        "    p = np.zeros(m)\n",
        "    h1 = sigmoid(np.dot(np.concatenate([np.ones((m, 1)), X], axis=1), Theta1.T))\n",
        "    h2 = sigmoid(np.dot(np.concatenate([np.ones((m, 1)), h1], axis=1), Theta2.T))\n",
        "    p = np.argmax(h2, axis=1)\n",
        "    return p"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rxMinI1Y_6AG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a37bc4e5-5932-46dc-ecb5-7b6e6a965b21"
      },
      "source": [
        "pred = predict(Theta1, Theta2, X[:,:])\n",
        "print(pred)\n",
        "print('Training Set Accuracy: %f' % (np.mean(pred == y[:]) * 100))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 0 0 ... 0 0 0]\n",
            "Training Set Accuracy: 58.316254\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Qrmea3i_hip",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6ffbc5f-5484-4a05-bf70-d4a9cf54eb64"
      },
      "source": [
        "pred = predict(Theta1, Theta2, Xp[:,:])\n",
        "print(pred)\n",
        "print('Testing Set Accuracy: %f' % (np.mean(pred == yp[:]) * 100))\n"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2 0 2 ... 0 0 0]\n",
            "Testing Set Accuracy: 58.141001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Después de realizar pruebas y manipular los siguientes parámetros para mejorar la precisión del modelo:\n",
        "\n",
        "Lambda (regularización de la función de costo para evitar sobreajustes).\n",
        "Maxiter (número de iteraciones que realizará el optimizer minimizer para una mejor convergencia).\n",
        "Hidden_layer_size (cantidad de perceptrones en la única capa intermedia.).\n",
        "\n",
        "Se logró alcanzar precisiones desde el 40% como el más bajo, hasta un máximo del 58%. Aunque esta precisión no sea muy alta ni la esperada, puede deberse a que el dataset de popularidad no muestra una linealidad o una relación muy marcada entre la popularidad de un artículo y sus características. No obstante, se pudo aplicar correctamente una red neuronal adaptándola a una clasificación utilizando la función sigmoide y su gradiente.\n",
        "\n",
        "Es importante mencionar que los valores de los hiperparámetros utilizados en el programa son aquellos con los que se obtuvo la mayor precisión según las pruebas realizadas. Se observó que la precisión aumenta si no se aplica regularización (lambda=0), pero es crucial aplicar regularización para evitar el sobreajuste. Aunque esto implique una ligera disminución en la precisión, se obtiene un modelo bien generalizado que puede ser más útil en escenarios del mundo real."
      ],
      "metadata": {
        "id": "fHRXyyLo_Onf"
      }
    }
  ]
}