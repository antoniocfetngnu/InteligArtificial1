{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/antoniocfetngnu/InteligArtificial1/blob/main/Clasificaciones/onevsAll/CalderonEAntonio_reg_log_onevsall.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Descripcion Dataset (Predicción de magnitud de terremotos)\n",
        "\n",
        "Este dataset nos entrega información pertinente a terremotos, siendo uno de esos datos la **magnitud** de un terremoto que se utilizará como **Label Y** **redondeandolos a valores enteros** para tener categorías comunes entre terremotos, las columnas que se tienen son:\n",
        "\n",
        "**Time**: Timestamp of the earthquake event.\n",
        "\n",
        "**Latitude**: Geographic coordinate specifying the north-south position.\n",
        "\n",
        "**Longitude**: Geographic coordinate specifying the east-west position.\n",
        "\n",
        "**Depth**: Depth of the earthquake in kilometers.\n",
        "\n",
        "**Mag**: Magnitude of the earthquake.\n",
        "\n",
        "**MagType**: Type of magnitude measurement.\n",
        "\n",
        "**Nst**: Number of seismic stations that reported the earthquake.\n",
        "\n",
        "**Gap**: The gap between different seismic stations' coverage.\n",
        "\n",
        "**Dmin**: Minimum distance to the earthquake epicenter for the nearest station.\n",
        "\n",
        "**Rms**: Root Mean Square of the earthquake's amplitude spectrum.\n",
        "\n",
        "**Net**: Network reporting the earthquake.\n",
        "\n",
        "**Id**: Unique identifier for the earthquake event.\n",
        "\n",
        "**Updated**: Timestamp indicating when the earthquake information was last updated.\n",
        "\n",
        "**Place**: Location description of the earthquake.\n",
        "\n",
        "**Type**: Type of seismic event (e.g., earthquake).\n",
        "\n",
        "**HorizontalError**: Horizontal error in location determination.\n",
        "\n",
        "**DepthError**: Error in depth determination.\n",
        "\n",
        "**MagError**: Error in magnitude determination.\n",
        "\n",
        "**MagNst**: Number of seismic stations used to calculate the magnitude.\n",
        "\n",
        "**Status**: Status of the earthquake event (e.g., reviewed).\n",
        "\n",
        "**LocationSource**: Source reporting the earthquake location.\n",
        "\n",
        "**MagSource**: Source reporting the earthquake magnitude.\n",
        "\n",
        "A continuación se realiza el ejercicio analizando el dataset, limpiandolo y dejarlo listo para hacer las predicciones onevsAll"
      ],
      "metadata": {
        "id": "e7G4pke97bX9"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xTjmiqnfSxgG"
      },
      "source": [
        "# Clasificación multiclase\n",
        "\n",
        "##Importacion de librerías\n",
        "\n",
        "Primero importamos todas las librerías que necesitamos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "id": "20Q29kX8SxgJ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy import optimize\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import random\n",
        "from sklearn.metrics import accuracy_score"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora importamos el dataset como dataframe de pandas"
      ],
      "metadata": {
        "id": "3Djzos60vbEa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('earthquakes_2023_global.csv')"
      ],
      "metadata": {
        "id": "JR9T_5roTXZz"
      },
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Limpieza y acomodo de Dataset\n",
        "Despues de analizar el dataset, debemos eliminar algunas columnas que no son necesarias (columnas de ID, columnas que tienen el mismo valor en todo,etc)\n",
        "\n",
        "##status\n",
        "En este caso **status** tiene el mismo valor en toda la columna, por tanto no es relevante y se elimina.\n",
        "\n",
        "##type\n",
        "Tambien **type**, ya que el objetivo es **predecir la magnitud de los terremotos**, en el dataset hay una minuscula (menores a 100 e incluso 10) datos que no corresponde a terremotos, por tanto como se decidió filtrar los datos a solo un mismo tipo (terremotos) entonces esta columna ya no tiene un valor variable por tanto también se eliminó.\n",
        "\n",
        "##Columnas con texto\n",
        "Las columnas a las que se les aplico **pd.factorize** son columnas las cuales tienen como valores texto y categorías, por tanto, lo que se hizo fue convertirlos a un equivalente numérico con un valor distinto para cada ocurrencia de distinto nombre, de esta forma seguirá categorizado pero numéricamente.\n",
        "\n",
        "Por tanto, se añadió nuevas columnas con el equivalente numérico, entonces las columnas fuente, también son eliminadas ya que tienen su representación numerica en nuevas columnas.\n",
        "\n",
        "##Columnas con formato ISO de fecha\n",
        "\n",
        "El dataset cuenta con 2 columnas, una que indica cuando se registró el terremoto y otra columna que indica cuando se actualizó el evento por ultima vez.\n",
        "\n",
        "En este caso, nos interesa extraer el mes donde ocurrio el evento, con la intención de predecir temporadas donde ocurren los terremotos, por tanto se extrae el mes de ambas columnas y se crean 2 nuevas columnas que los contienen, y se eliminan las columnas originales.\n",
        "\n",
        "##Salida (label) 'mag'\n",
        "\n",
        "Para la salida lo que se hizo fue \"redondear\" las magnitudes a su entero mas cercano (priorizando cuando sea el caso de .5 al entero superior) para poder tener categorías útiles para el ejercicio sin afectar mucho a la validez de los datos."
      ],
      "metadata": {
        "id": "v93DcOHAvmoR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {
        "id": "hhRjL2ptSxgK"
      },
      "outputs": [],
      "source": [
        "pd.set_option('display.max_columns', None)\n",
        "df.drop('type', axis=1, inplace=True)\n",
        "df.drop('status', axis=1, inplace=True)\n",
        "df['magType_code'] = pd.factorize(df['magType'])[0] + 1\n",
        "df['location_code'] = pd.factorize(df['locationSource'])[0] + 1\n",
        "df['magSource_code'] = pd.factorize(df['magSource'])[0] + 1\n",
        "df['net_code'] = pd.factorize(df['net'])[0]+1\n",
        "df.drop('magType', axis=1, inplace=True)\n",
        "df.drop('locationSource', axis=1, inplace=True)\n",
        "df.drop('magSource', axis=1, inplace=True)\n",
        "df.drop('id', axis=1, inplace=True)\n",
        "df.drop('net', axis=1, inplace=True)\n",
        "\n",
        "#convertir el formato ISO a fecha con datetime de pandas\n",
        "df['time'] = pd.to_datetime(df['time'].str.replace('Z', '+00:00'), format='%Y-%m-%dT%H:%M:%S.%f%z')\n",
        "df['updated'] = pd.to_datetime(df['updated'].str.replace('Z', '+00:00'), format='%Y-%m-%dT%H:%M:%S.%f%z')\n",
        "\n",
        "# Extraer el mes de la fecha y crear una nueva columna \"Month\"\n",
        "df['Month'] = df['time'].dt.month\n",
        "\n",
        "df['MonthUpdated'] = df['updated'].dt.month\n",
        "\n",
        "df.drop('time', axis=1, inplace=True)\n",
        "df.drop('updated', axis=1, inplace=True)\n",
        "df.drop('place', axis=1, inplace=True)\n",
        "#comprobada la increible correlacion entre location_code, magSource_code y net_code entonces se decidio mantener solo una ya que expresan lo mismo\n",
        "df.drop('location_code', axis=1, inplace=True)\n",
        "df.drop('net_code', axis=1, inplace=True)\n",
        "#redondeamos y en los casos en que sea .5 se redonde al mayor no al menor\n",
        "df['mag'] = df['mag'].apply(lambda x: np.ceil(x) if x - int(x) == 0.5 else np.round(x))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Una vez, acomodado los tipos de datos de todas las columnas satisfactoriamente, aplicamos **df.describe()** para que nos muestre un resumen de todas nuestras columnas, mediante esto podremos identificar si algunas columnas tienen NANs (datos faltantes) o si ya estuviera preparado para la regresión logistica onevsAll"
      ],
      "metadata": {
        "id": "tfEJGcc1yWFQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.describe())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bDNmvKfPtpMX",
        "outputId": "f5e545f7-7a76-4605-8ec6-f33c2fd8aea3"
      },
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "           latitude     longitude         depth           mag           nst  \\\n",
            "count  26642.000000  26642.000000  26642.000000  26642.000000  25227.000000   \n",
            "mean      16.852798    -11.487497     67.491224      4.084341     42.571332   \n",
            "std       30.389200    130.053399    116.762456      0.839589     37.662352   \n",
            "min      -65.849700   -179.998700     -3.370000      3.000000      0.000000   \n",
            "25%       -6.415275   -149.608650     10.000000      3.000000     19.000000   \n",
            "50%       18.884167    -64.811833     21.998000      4.000000     30.000000   \n",
            "75%       41.827950    126.965100     66.833000      5.000000     52.000000   \n",
            "max       86.593900    179.999400    681.238000      8.000000    423.000000   \n",
            "\n",
            "                gap          dmin           rms  horizontalError  \\\n",
            "count  25225.000000  24776.000000  26642.000000     25093.000000   \n",
            "mean     124.930971      2.692908      0.581575         7.017267   \n",
            "std       67.430145      4.043568      0.256276         4.072365   \n",
            "min        8.000000      0.000000      0.010000         0.000000   \n",
            "25%       73.000000      0.612000      0.410000         4.140000   \n",
            "50%      111.000000      1.579000      0.590000         7.060000   \n",
            "75%      165.000000      3.172000      0.750000         9.730000   \n",
            "max      350.000000     50.820000      1.880000        99.000000   \n",
            "\n",
            "         depthError      magError        magNst  magType_code  magSource_code  \\\n",
            "count  26642.000000  24970.000000  25065.000000  26642.000000    26642.000000   \n",
            "mean       4.475056      0.122735     33.315939      2.414158        1.756662   \n",
            "std        4.451649      0.102271     48.022567      2.303647        1.921662   \n",
            "min        0.000000      0.000000      0.000000      1.000000        1.000000   \n",
            "25%        1.848000      0.080000     10.000000      2.000000        1.000000   \n",
            "50%        2.019000      0.111000     18.000000      2.000000        1.000000   \n",
            "75%        6.669000      0.150000     36.000000      2.000000        1.000000   \n",
            "max       60.670000      4.490000    884.000000     15.000000       19.000000   \n",
            "\n",
            "              Month  MonthUpdated  \n",
            "count  26642.000000  26642.000000  \n",
            "mean       6.429697      8.231552  \n",
            "std        3.489219      3.109530  \n",
            "min        1.000000      1.000000  \n",
            "25%        3.000000      5.000000  \n",
            "50%        6.000000      8.000000  \n",
            "75%       10.000000     11.000000  \n",
            "max       12.000000     12.000000  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gracias a la respuesta anterior, se puede observar que el atributo **count** de algunas columnas no coincide, eso significa que su cantidad de valores es menor posiblemente debido a la presencia de casillas vacías **NANs**, por tanto debemos ver la forma de tratar estos campos."
      ],
      "metadata": {
        "id": "QBVA_JqOyyqt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nans_en_columna = df['latitude'].isnull().sum()\n",
        "print(f\"Cantidad de NaNs en la columna 'latitude': {nans_en_columna}\")\n",
        "nans_en_columna1 = df['longitude'].isnull().sum()\n",
        "print(f\"Cantidad de NaNs en la columna 'longitude': {nans_en_columna1}\")\n",
        "nans_en_columna2 = df['depth'].isnull().sum()\n",
        "print(f\"Cantidad de NaNs en la columna 'depth': {nans_en_columna2}\")\n",
        "nans_en_columna3 = df['mag'].isnull().sum()\n",
        "print(f\"Cantidad de NaNs en la columna 'mag': {nans_en_columna3}\")\n",
        "nans_en_columna4 = df['nst'].isnull().sum()\n",
        "print(f\"Cantidad de NaNs en la columna 'nst': {nans_en_columna4}\")\n",
        "nans_en_columna5 = df['gap'].isnull().sum()\n",
        "print(f\"Cantidad de NaNs en la columna 'gap': {nans_en_columna5}\")\n",
        "nans_en_columna6 = df['dmin'].isnull().sum()\n",
        "print(f\"Cantidad de NaNs en la columna 'dmin': {nans_en_columna6}\")\n",
        "nans_en_columna7 = df['rms'].isnull().sum()\n",
        "print(f\"Cantidad de NaNs en la columna 'rms': {nans_en_columna7}\")\n",
        "nans_en_columna8 = df['horizontalError'].isnull().sum()\n",
        "print(f\"Cantidad de NaNs en la columna 'horizontalError': {nans_en_columna8}\")\n",
        "nans_en_columna9 = df['depthError'].isnull().sum()\n",
        "print(f\"Cantidad de NaNs en la columna 'depthError': {nans_en_columna9}\")\n",
        "nans_en_columna10 = df['magError'].isnull().sum()\n",
        "print(f\"Cantidad de NaNs en la columna 'magError': {nans_en_columna10}\")\n",
        "nans_en_columna11 = df['magNst'].isnull().sum()\n",
        "print(f\"Cantidad de NaNs en la columna 'magNst': {nans_en_columna11}\")\n",
        "nans_en_columna12 = df['magType_code'].isnull().sum()\n",
        "print(f\"Cantidad de NaNs en la columna 'magType_code': {nans_en_columna12}\")\n",
        "nans_en_columna13 = df['magSource_code'].isnull().sum()\n",
        "print(f\"Cantidad de NaNs en la columna 'magSource_code': {nans_en_columna13}\")\n",
        "nans_en_columna14 = df['Month'].isnull().sum()\n",
        "print(f\"Cantidad de NaNs en la columna 'Month': {nans_en_columna14}\")\n",
        "nans_en_columna15 = df['MonthUpdated'].isnull().sum()\n",
        "print(f\"Cantidad de NaNs en la columna 'MonthUpdated': {nans_en_columna15}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ojM-o4h1tpZM",
        "outputId": "be63af74-be9a-4f07-ee7d-2c2f9033cdfb"
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cantidad de NaNs en la columna 'latitude': 0\n",
            "Cantidad de NaNs en la columna 'longitude': 0\n",
            "Cantidad de NaNs en la columna 'depth': 0\n",
            "Cantidad de NaNs en la columna 'mag': 0\n",
            "Cantidad de NaNs en la columna 'nst': 1415\n",
            "Cantidad de NaNs en la columna 'gap': 1417\n",
            "Cantidad de NaNs en la columna 'dmin': 1866\n",
            "Cantidad de NaNs en la columna 'rms': 0\n",
            "Cantidad de NaNs en la columna 'horizontalError': 1549\n",
            "Cantidad de NaNs en la columna 'depthError': 0\n",
            "Cantidad de NaNs en la columna 'magError': 1672\n",
            "Cantidad de NaNs en la columna 'magNst': 1577\n",
            "Cantidad de NaNs en la columna 'magType_code': 0\n",
            "Cantidad de NaNs en la columna 'magSource_code': 0\n",
            "Cantidad de NaNs en la columna 'Month': 0\n",
            "Cantidad de NaNs en la columna 'MonthUpdated': 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gracias a la ejecución del código anterior, podemos verificar que precisamente el problema es la ausencia de datos en ciertas columnas de los ejemplos del dataset."
      ],
      "metadata": {
        "id": "Uz_lYr_wz_9L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Llenado de valores NAN con  .mean()\n",
        "Uno de los métodos para tratar estos valores NAN, es aplicar el .mean() (promedio) de los valores de la columna y rellenar los NAN con dicho valor, para este caso, se utilizará esta técnica, a las columnas identificadas con NANs anteriormente."
      ],
      "metadata": {
        "id": "WgE03Q1LzH8e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mean_nst = df[\"nst\"].mean()\n",
        "df[\"nst\"] = df[\"nst\"].fillna(mean_nst)\n",
        "\n",
        "mean_gap = df[\"gap\"].mean()\n",
        "df[\"gap\"] = df[\"gap\"].fillna(mean_gap)\n",
        "\n",
        "mean_dmin = df[\"dmin\"].mean()\n",
        "df[\"dmin\"] = df[\"dmin\"].fillna(mean_dmin)\n",
        "\n",
        "mean_hError = df[\"horizontalError\"].mean()\n",
        "df[\"horizontalError\"] = df[\"horizontalError\"].fillna(mean_hError)\n",
        "\n",
        "mean_magError= df[\"magError\"].mean()\n",
        "df[\"magError\"] = df[\"magError\"].fillna(mean_magError)\n",
        "\n",
        "mean_magNst= df[\"magNst\"].mean()\n",
        "df[\"magNst\"] = df[\"magNst\"].fillna(mean_magNst)"
      ],
      "metadata": {
        "id": "j4FmGoaAtw6E"
      },
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Comprobación\n",
        "Una vez aplicada la media, comprobamos nuevamente si aún contamos con NaNs en las columnas:"
      ],
      "metadata": {
        "id": "4SvRAniZ0P--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nans_en_columna = df['latitude'].isnull().sum()\n",
        "print(f\"Cantidad de NaNs en la columna 'latitude': {nans_en_columna}\")\n",
        "nans_en_columna1 = df['longitude'].isnull().sum()\n",
        "print(f\"Cantidad de NaNs en la columna 'longitude': {nans_en_columna1}\")\n",
        "nans_en_columna2 = df['depth'].isnull().sum()\n",
        "print(f\"Cantidad de NaNs en la columna 'depth': {nans_en_columna2}\")\n",
        "nans_en_columna3 = df['mag'].isnull().sum()\n",
        "print(f\"Cantidad de NaNs en la columna 'mag': {nans_en_columna3}\")\n",
        "nans_en_columna4 = df['nst'].isnull().sum()\n",
        "print(f\"Cantidad de NaNs en la columna 'nst': {nans_en_columna4}\")\n",
        "nans_en_columna5 = df['gap'].isnull().sum()\n",
        "print(f\"Cantidad de NaNs en la columna 'gap': {nans_en_columna5}\")\n",
        "nans_en_columna6 = df['dmin'].isnull().sum()\n",
        "print(f\"Cantidad de NaNs en la columna 'dmin': {nans_en_columna6}\")\n",
        "nans_en_columna7 = df['rms'].isnull().sum()\n",
        "print(f\"Cantidad de NaNs en la columna 'rms': {nans_en_columna7}\")\n",
        "nans_en_columna8 = df['horizontalError'].isnull().sum()\n",
        "print(f\"Cantidad de NaNs en la columna 'horizontalError': {nans_en_columna8}\")\n",
        "nans_en_columna9 = df['depthError'].isnull().sum()\n",
        "print(f\"Cantidad de NaNs en la columna 'depthError': {nans_en_columna9}\")\n",
        "nans_en_columna10 = df['magError'].isnull().sum()\n",
        "print(f\"Cantidad de NaNs en la columna 'magError': {nans_en_columna10}\")\n",
        "nans_en_columna11 = df['magNst'].isnull().sum()\n",
        "print(f\"Cantidad de NaNs en la columna 'magNst': {nans_en_columna11}\")\n",
        "nans_en_columna12 = df['magType_code'].isnull().sum()\n",
        "print(f\"Cantidad de NaNs en la columna 'magType_code': {nans_en_columna12}\")\n",
        "nans_en_columna13 = df['magSource_code'].isnull().sum()\n",
        "print(f\"Cantidad de NaNs en la columna 'magSource_code': {nans_en_columna13}\")\n",
        "nans_en_columna14 = df['Month'].isnull().sum()\n",
        "print(f\"Cantidad de NaNs en la columna 'Month': {nans_en_columna14}\")\n",
        "nans_en_columna15 = df['MonthUpdated'].isnull().sum()\n",
        "print(f\"Cantidad de NaNs en la columna 'MonthUpdated': {nans_en_columna15}\")\n",
        "print(df.describe())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D4lj1c-otzGS",
        "outputId": "df85b370-6d67-4cb9-88ca-813f59137e28"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cantidad de NaNs en la columna 'latitude': 0\n",
            "Cantidad de NaNs en la columna 'longitude': 0\n",
            "Cantidad de NaNs en la columna 'depth': 0\n",
            "Cantidad de NaNs en la columna 'mag': 0\n",
            "Cantidad de NaNs en la columna 'nst': 0\n",
            "Cantidad de NaNs en la columna 'gap': 0\n",
            "Cantidad de NaNs en la columna 'dmin': 0\n",
            "Cantidad de NaNs en la columna 'rms': 0\n",
            "Cantidad de NaNs en la columna 'horizontalError': 0\n",
            "Cantidad de NaNs en la columna 'depthError': 0\n",
            "Cantidad de NaNs en la columna 'magError': 0\n",
            "Cantidad de NaNs en la columna 'magNst': 0\n",
            "Cantidad de NaNs en la columna 'magType_code': 0\n",
            "Cantidad de NaNs en la columna 'magSource_code': 0\n",
            "Cantidad de NaNs en la columna 'Month': 0\n",
            "Cantidad de NaNs en la columna 'MonthUpdated': 0\n",
            "           latitude     longitude         depth           mag           nst  \\\n",
            "count  26642.000000  26642.000000  26642.000000  26642.000000  26642.000000   \n",
            "mean      16.852798    -11.487497     67.491224      4.084341     42.571332   \n",
            "std       30.389200    130.053399    116.762456      0.839589     36.648514   \n",
            "min      -65.849700   -179.998700     -3.370000      3.000000      0.000000   \n",
            "25%       -6.415275   -149.608650     10.000000      3.000000     19.000000   \n",
            "50%       18.884167    -64.811833     21.998000      4.000000     32.000000   \n",
            "75%       41.827950    126.965100     66.833000      5.000000     50.000000   \n",
            "max       86.593900    179.999400    681.238000      8.000000    423.000000   \n",
            "\n",
            "                gap          dmin           rms  horizontalError  \\\n",
            "count  26642.000000  26642.000000  26642.000000     26642.000000   \n",
            "mean     124.930971      2.692908      0.581575         7.017267   \n",
            "std       65.612383      3.899387      0.256276         3.952201   \n",
            "min        8.000000      0.000000      0.010000         0.000000   \n",
            "25%       75.000000      0.666475      0.410000         4.350000   \n",
            "50%      116.000000      1.760000      0.590000         7.017267   \n",
            "75%      160.000000      3.020750      0.750000         9.550000   \n",
            "max      350.000000     50.820000      1.880000        99.000000   \n",
            "\n",
            "         depthError      magError        magNst  magType_code  magSource_code  \\\n",
            "count  26642.000000  26642.000000  26642.000000  26642.000000    26642.000000   \n",
            "mean       4.475056      0.122735     33.315939      2.414158        1.756662   \n",
            "std        4.451649      0.099010     46.579551      2.303647        1.921662   \n",
            "min        0.000000      0.000000      0.000000      1.000000        1.000000   \n",
            "25%        1.848000      0.082000     11.000000      2.000000        1.000000   \n",
            "50%        2.019000      0.115000     19.000000      2.000000        1.000000   \n",
            "75%        6.669000      0.148000     34.000000      2.000000        1.000000   \n",
            "max       60.670000      4.490000    884.000000     15.000000       19.000000   \n",
            "\n",
            "              Month  MonthUpdated  \n",
            "count  26642.000000  26642.000000  \n",
            "mean       6.429697      8.231552  \n",
            "std        3.489219      3.109530  \n",
            "min        1.000000      1.000000  \n",
            "25%        3.000000      5.000000  \n",
            "50%        6.000000      8.000000  \n",
            "75%       10.000000     11.000000  \n",
            "max       12.000000     12.000000  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Como se puede observar se aplicó correctamente y ya no tenemos NANs en las columnas"
      ],
      "metadata": {
        "id": "lyVyd6M5zn_z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Separación en entrenamiento y prueba\n",
        "Ahora, con el dataset listo, debemos separar los datos de manera coherente\n",
        "##Tomar el 20% de los datos de cada ocurrencia en Y(label)\n",
        "Entonces, utilizando train_test_split con stratify y test_size=0.2 aseguramos que se tome el 20% de cada ocurrencia en Y, además la semilla aleatoria es 42, por si quisieramos replicar la pseudo aleatorieadad al correr nuevamente el código.\n",
        "\n",
        "También se realizó la verificación de proporciones de los datos en y_train y y_test y vemos que la proporcion es similar, en algunos casos no es exacto ya que por ejemplo para terremotos de magnitud 8, no se tiene muchos datos (magnitud 8 datos <100)\n"
      ],
      "metadata": {
        "id": "8qJVzScl0Z9i"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {
        "id": "V2C82bqdSxgL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00a7b6d8-0847-4374-b6fb-154f1f77c19e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   latitude  longitude   depth   nst    gap    dmin   rms  horizontalError  \\\n",
            "0   52.0999   178.5218  82.770  14.0  139.0  0.8700  0.18             8.46   \n",
            "1    7.1397   126.7380  79.194  32.0  104.0  1.1520  0.47             5.51   \n",
            "2   19.1631   -66.5251  24.000  23.0  246.0  0.8479  0.22             0.91   \n",
            "3   -4.7803   102.7675  63.787  17.0  187.0  0.4570  0.51            10.25   \n",
            "4   53.3965  -166.9417  10.000  19.0  190.0  0.4000  0.31             1.41   \n",
            "\n",
            "   depthError  magError  magNst  magType_code  magSource_code  Month  \\\n",
            "0      21.213     0.097    14.0             1               1      1   \n",
            "1       7.445     0.083    43.0             2               1      1   \n",
            "2      15.950     0.090    16.0             3               2      1   \n",
            "3       6.579     0.238     5.0             2               1      1   \n",
            "4       1.999     0.085    18.0             1               1      1   \n",
            "\n",
            "   MonthUpdated  \n",
            "0             3  \n",
            "1             3  \n",
            "2             3  \n",
            "3             3  \n",
            "4             3  \n",
            "--------------------\n",
            "0    3.0\n",
            "1    5.0\n",
            "2    4.0\n",
            "3    4.0\n",
            "4    3.0\n",
            "Name: mag, dtype: float64\n",
            "Proporción de clases en y_train:\n",
            "4.0    0.357622\n",
            "5.0    0.333177\n",
            "3.0    0.289823\n",
            "6.0    0.017360\n",
            "7.0    0.001783\n",
            "8.0    0.000235\n",
            "Name: mag, dtype: float64\n",
            "\n",
            "Proporción de clases en y_test:\n",
            "4.0    0.357666\n",
            "5.0    0.333271\n",
            "3.0    0.289735\n",
            "6.0    0.017452\n",
            "7.0    0.001689\n",
            "8.0    0.000188\n",
            "Name: mag, dtype: float64\n"
          ]
        }
      ],
      "source": [
        "y1 = df['mag']\n",
        "X1 = df.drop(columns=['mag'])\n",
        "\n",
        "print(X1.head())\n",
        "print('-'*20)\n",
        "print(y1.head())\n",
        "X_train, X_test, y_train, y_test = train_test_split(X1, y1, test_size=0.2, stratify=y1, random_state=42)\n",
        "\n",
        "# Verificar la proporción de clases en los conjuntos de entrenamiento y prueba\n",
        "print(\"Proporción de clases en y_train:\")\n",
        "print(y_train.value_counts(normalize=True))\n",
        "\n",
        "print(\"\\nProporción de clases en y_test:\")\n",
        "print(y_test.value_counts(normalize=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Una vez separados los datos en entrenamiento y prueba, debemos **normalizar las X**, entonces instanciamos un **objeto scaler**, mediante el cual podemos **estandarizar**(usar la media y la desviacion estandar para **normalizar**) como usualmente hacíamos con la función definida por nosotros(ojo: se realizó una **comparación** de las salidas obtenidas **con este método** y con la **función original del ejemplo** y se obtiene exactamente el **mismo resultado**, pero sin embargo de esta forma se tiene la **ventaja** de estandarizar las **X de prueba** con la desviación y media de **X de entrenamiento** para que ambos posean la **misma estandarización aplicada**."
      ],
      "metadata": {
        "id": "kB9mOm3-1W4X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalizar características solo en los conjuntos de entrenamiento y prueba\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "print(X_train_scaled)\n",
        "print('-'*30)\n",
        "print(X_test_scaled)\n",
        "\n",
        "# Configurar la matriz adecuadamente, y agregar una columna de unos que corresponde al termino de intercepción.\n",
        "m, n = X_train_scaled.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GTbKHJAGUnGm",
        "outputId": "25def175-55a7-4e6e-b1d2-c7143639e7f1"
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 0.0568953   1.21179003 -0.49256614 ... -0.39275517  1.3093833\n",
            "   0.88942869]\n",
            " [-2.39244322 -0.12026503 -0.26384398 ... -0.39275517  0.45001357\n",
            "   0.56832437]\n",
            " [-0.30944051  1.0597022  -0.49166697 ... -0.39275517 -0.98226932\n",
            "  -1.03719725]\n",
            " ...\n",
            " [-2.44207669 -0.43145249 -0.49256614 ... -0.39275517 -0.40935617\n",
            "  -0.07388428]\n",
            " [ 0.10033819  1.20450601 -0.49256614 ... -0.39275517  1.3093833\n",
            "   1.21053302]\n",
            " [ 0.034744   -0.42539004 -0.46070997 ...  0.1310085  -0.69581274\n",
            "  -1.35830157]]\n",
            "------------------------------\n",
            "[[-0.85158282  1.29090079 -0.49256614 ... -0.39275517 -1.55518248\n",
            "  -1.67940589]\n",
            " [-0.78476283  1.08786482  0.24353403 ... -0.39275517  0.73647014\n",
            "   0.88942869]\n",
            " [ 1.2039696  -1.16862935 -0.27847897 ... -0.39275517 -1.55518248\n",
            "  -1.67940589]\n",
            " ...\n",
            " [ 1.1739105  -0.15477281 -0.49256614 ... -0.39275517  0.45001357\n",
            "   0.56832437]\n",
            " [ 1.16919748 -1.21403817 -0.27847897 ... -0.39275517 -1.2687259\n",
            "  -1.35830157]\n",
            " [ 1.13291152 -1.27656631 -0.27847897 ... -0.39275517 -1.2687259\n",
            "  -1.35830157]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ya que el método onevsAll es la aplicación de reg. Logística pero para varias categorías, debemos definir la sigmoide como hicimos en el caso anterior:"
      ],
      "metadata": {
        "id": "0yBvc9Y22HyF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {
        "id": "0sB9Kyi8SxgN"
      },
      "outputs": [],
      "source": [
        "def sigmoid(z):\n",
        "\n",
        "    return 1.0 / (1.0 + np.exp(-z))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se define la función de costo pero en este caso aumentando lambda para evitar sobreajustes y por tanto que el modelo no tome en cuenta o minimice el ruido de datos muy dispersos."
      ],
      "metadata": {
        "id": "AjJrgT9m2R5P"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {
        "id": "w5S0OOswSxgN"
      },
      "outputs": [],
      "source": [
        "def lrCostFunction(theta, X, y, lambda_):\n",
        "\n",
        "    m = y.size\n",
        "\n",
        "    # convierte las etiquetas a valores enteros si son boleanos\n",
        "    if y.dtype == bool:\n",
        "        y = y.astype(int)\n",
        "\n",
        "    J = 0\n",
        "    grad = np.zeros(theta.shape)\n",
        "\n",
        "    h = sigmoid(X.dot(theta.T))\n",
        "    # print('valor de h ================')\n",
        "    # print(h)\n",
        "\n",
        "    temp = theta\n",
        "    temp[0] = 0\n",
        "\n",
        "    J = (1 / m) * np.sum(-y.dot(np.log(h)) - (1 - y).dot(np.log(1 - h))) + (lambda_ / (2 * m)) * np.sum(np.square(temp))\n",
        "\n",
        "    grad = (1 / m) * (h - y).dot(X)\n",
        "#     theta = theta - (alpha / m) * (h - y).dot(X)\n",
        "    grad = grad + (lambda_ / m) * temp\n",
        "    # print('valores intermedios J----------------')\n",
        "    # print(J)\n",
        "    # print(grad)\n",
        "    return J, grad"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definimos la función de oneVsAll y en este caso no usaremos nuestra función de descenso por la gradiente, sino usaremos **optimize.minimize** con su propio **metodo 'CG'**que **reemplaza** al descenso por la gradiente que usabamos en anteriores ejercicios.\n",
        "\n",
        "Además, debido a que usaremos **np.arange**, los valores de c irán variando de 0 hasta el rango máximo -1 (num_labels), la **ventaja de arange** a diferencia de **range de python** sin numpy, puede manejar **vectores** y en este caso como calcularemos las thithas de varias categorías nos es muy util.\n",
        "\n",
        "Sin embargo, debido a que se compara **y==c** en los parámetros de **optimize.minimize**, entonces lo que hará es buscar **coincidencias** de **y** a la **c** actual y **tomará solo las coincidencias** para entrenar a ese conjunto, de ahi el nombre de onevsAll, y así sucesivamente hasta salir del ciclo for. Es por eso que **las magnitudes de salida de nuestra Y debemos acomodarlas** al mismo rango de **c** para que pueda coincidir **y==c**"
      ],
      "metadata": {
        "id": "Cq1Slqd92ggN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {
        "id": "V0rOw5qhSxgN"
      },
      "outputs": [],
      "source": [
        "def oneVsAll(X, y, num_labels, lambda_):\n",
        "\n",
        "    # algunas variables utiles\n",
        "    m, n = X.shape\n",
        "    print('------------------')\n",
        "    print('valor n:',n)\n",
        "    print('valor m:', n)\n",
        "    all_theta = np.zeros((num_labels, n + 1))\n",
        "\n",
        "    # Agrega unos a la matriz X\n",
        "    X = np.concatenate([np.ones((m, 1)), X], axis=1)\n",
        "\n",
        "    for c in np.arange(num_labels):\n",
        "        print(\"valor de c:\",c)\n",
        "        initial_theta = np.zeros(n + 1)\n",
        "        options = {'maxiter': 50}\n",
        "        res = optimize.minimize(lrCostFunction,\n",
        "                                initial_theta,\n",
        "                                (X, (y == c), lambda_),\n",
        "                                jac=True,\n",
        "                                method='CG',\n",
        "                                options=options)\n",
        "\n",
        "        all_theta[c] = res.x\n",
        "\n",
        "    return all_theta"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Entonces, primero debemos verificar cuantas categorías tenemos en nuestro **label Y**:"
      ],
      "metadata": {
        "id": "3-X2gNGD38n3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(y_train.unique())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VleTK1xo4B6j",
        "outputId": "345f029b-e9ee-446c-9d3e-0c2bdd96493f"
      },
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[5. 4. 3. 6. 7. 8.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gracias a lo anterior podemos ver que tenemos **6 categorías** (magnitudes de terremotos) pero no van del 0 al 5, por tanto **mapearemos** de 0 a 5 para **coincidir con los valores de c**"
      ],
      "metadata": {
        "id": "gYR5XaqU4Jtm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mapeo_etiquetas = {3: 0, 4: 1, 5: 2, 6: 3, 7: 4, 8: 5}\n",
        "y_train_mapped = y_train.map(mapeo_etiquetas)       #mapeamos a valores del 0 al 5 para el onevsAll\n",
        "y_train_mapped_array = y_train_mapped.values        #lo convertimos en un numpy array para poder usarlo en nuestras funciones\n",
        "\n",
        "num_labels = len(np.unique(y_train_mapped_array))\n",
        "print(num_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ft-RScRdZATK",
        "outputId": "06b480b3-b045-49cc-aa96-a83fcac92097"
      },
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora, ya estamos preparados para calculas las thitas correspondientes mediante **oneVsAll** para nuestro dataset y tenemos:"
      ],
      "metadata": {
        "id": "kWDKg5AU4et9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {
        "id": "w6JbsLLMSxgO",
        "outputId": "c1ff6544-0faf-4504-b27f-fced547fdbcd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------\n",
            "valor n: 15\n",
            "valor m: 15\n",
            "valor de c: 0\n",
            "valor de c: 1\n",
            "valor de c: 2\n",
            "valor de c: 3\n",
            "valor de c: 4\n",
            "valor de c: 5\n",
            "------------------------------------Salidaaaaaaaaa\n",
            "(6, 16)\n",
            "[[-4.70023641e+00  2.20498833e+00 -9.44600804e-01 -8.31399694e-01\n",
            "  -1.85976970e+00  3.12250308e-01 -3.05387381e+00 -1.02640606e+00\n",
            "  -2.91947748e-01 -1.89349308e-01 -1.96925586e-02  5.52314355e-01\n",
            "  -8.56346369e-01  8.17696319e-01  8.42866992e-01 -8.59869903e-01]\n",
            " [-8.09385132e-01  6.13282326e-02  4.07183762e-01  4.93254405e-01\n",
            "  -2.59059718e-01 -1.97679447e-01 -1.22966217e-01  2.21590705e-01\n",
            "   8.39279474e-03  2.57076239e-01  3.48477087e-01 -9.00501722e-01\n",
            "  -2.34093090e-01 -2.61844322e-01 -7.00353011e-01  6.55264809e-01]\n",
            " [-1.27285537e+00 -9.15591631e-01  2.82074652e-01 -6.22722309e-01\n",
            "   3.84178480e-01 -5.83397331e-01  8.91943722e-02  3.25225058e-01\n",
            "   4.89554866e-01 -2.03456768e-01 -1.02828911e+00  4.38209886e-01\n",
            "  -2.65232839e-01 -4.25518346e-01  2.82455108e-01 -2.23430247e-01]\n",
            " [-1.01007245e+01 -8.12406816e-01  3.18663104e-01 -2.70514365e-02\n",
            "   2.45248981e-01 -6.57345152e-01  4.45922421e-02  2.59818437e-01\n",
            "   4.50929538e-01 -1.35836392e+00 -3.68968639e+00  2.13015448e-01\n",
            "   1.03751497e+00 -1.15321984e+00  4.47040817e-01 -1.87749263e-01]\n",
            " [-1.62722323e+01 -6.36301776e-01  2.12578255e-02  1.93976960e-01\n",
            "   1.58628584e-01 -3.15974080e+00 -5.33528750e-01  4.56029749e-02\n",
            "   8.67693662e-02 -1.20628425e+00 -2.56018379e+00  3.71993366e-02\n",
            "   1.38805284e+00 -2.08012393e-01 -5.62024443e-01  7.97272154e-01]\n",
            " [-1.45997863e+01 -5.02864870e-01  3.48637818e-01 -2.98774131e-01\n",
            "   3.37399663e-01 -1.83173601e+00  4.73313698e-02  7.46781352e-01\n",
            "   1.18032694e-01  7.23027195e-01 -2.34081292e+00  2.64181262e-01\n",
            "   8.93559928e-01  2.92101514e-01 -9.36408330e-01  7.07736909e-01]]\n"
          ]
        }
      ],
      "source": [
        "lambda_ = 0.1\n",
        "all_theta = oneVsAll(X_train_scaled, y_train_mapped_array, num_labels, lambda_)\n",
        "print('------------------------------------Salidaaaaaaaaa')\n",
        "print(all_theta.shape)\n",
        "\n",
        "print(all_theta)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {
        "id": "PjFoFe1bSxgO"
      },
      "outputs": [],
      "source": [
        "def predictOneVsAll(all_theta, X):\n",
        "\n",
        "    m = X.shape[0];\n",
        "    num_labels = all_theta.shape[0]\n",
        "\n",
        "    p = np.zeros(m)\n",
        "\n",
        "    # Add ones to the X data matrix\n",
        "    X = np.concatenate([np.ones((m, 1)), X], axis=1)\n",
        "    p = np.argmax(sigmoid(X.dot(all_theta.T)), axis = 1)\n",
        "\n",
        "    return p"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Una vez obtenidas las thitas, es momento de realizar las predicciones, primero definimos la funcion que filtra la concordancia de clases con su **máximo** (para saber qué conjunto de thitas corresponde a qué **clase de Y**(categoria)).\n",
        "\n",
        "Para realizar esto debemos usar la X de prueba normalizada, entonces:"
      ],
      "metadata": {
        "id": "eomIkVEy5XRs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {
        "id": "mE7v5cglSxgO",
        "outputId": "ab61b393-7673-4929-e23e-e26c5ab547fc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(5329, 15)\n",
            "[2 1 0 ... 2 0 0]\n"
          ]
        }
      ],
      "source": [
        "print(X_test_scaled.shape)\n",
        "predicciones = predictOneVsAll(all_theta, X_test_scaled)\n",
        "print(predicciones)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Las predicciones (Y predicha) se guarda en **predicciones**, pero este se encuentra en formato **array de numpy**, por tanto para sacar un **accuracy score** correcto, ambos y de prueba y predicciones deben ser del mismo tipo de datos, en este caso es mas facil **volver a predicciones en un dataframe de solo una columna** ya que **y_test es un dataframe** que aún no se había transformado a un array de numpy.\n",
        "\n",
        "###OJO IMPORTANTE\n",
        "Ojo, no hay que olvidar que **mapeamos los datos de nuestra y de entrenamiento** para que **coincidan** con los valores de **c**, entonces, para hacer una **correcta comparación** de las y predichas mapeadas con las y de prueba debemos **mapear inversamente** las **y predichas** para que tengan sus valores **originales**. Entonces:"
      ],
      "metadata": {
        "id": "SYJUz7r-5zn8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mapeo_etiquetas_reverse = {v: k for k, v in mapeo_etiquetas.items()}\n",
        "\n",
        "predicciones_df = pd.DataFrame({'Predicciones': predicciones})\n",
        "\n",
        "# Aplicar el mapeo inverso a las predicciones del DataFrame\n",
        "predicciones_df['Predicciones'] = predicciones_df['Predicciones'].map(mapeo_etiquetas_reverse)\n",
        "\n",
        "print(predicciones_df)\n",
        "y_test_renumerado = y_test.reset_index(drop=True)\n",
        "print(y_test_renumerado)\n",
        "\n",
        "precision = accuracy_score(y_test, predicciones_df['Predicciones'])\n",
        "print(\"Precisión del modelo:\", precision)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rO7aEpuNu4A_",
        "outputId": "806af62e-0b6f-452d-81eb-f07832c42e95"
      },
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      Predicciones\n",
            "0                5\n",
            "1                4\n",
            "2                3\n",
            "3                5\n",
            "4                3\n",
            "...            ...\n",
            "5324             3\n",
            "5325             5\n",
            "5326             5\n",
            "5327             3\n",
            "5328             3\n",
            "\n",
            "[5329 rows x 1 columns]\n",
            "0       5.0\n",
            "1       4.0\n",
            "2       3.0\n",
            "3       5.0\n",
            "4       3.0\n",
            "       ... \n",
            "5324    3.0\n",
            "5325    4.0\n",
            "5326    5.0\n",
            "5327    3.0\n",
            "5328    3.0\n",
            "Name: mag, Length: 5329, dtype: float64\n",
            "Precisión del modelo: 0.7729405141677613\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Como se puede ver, el modelo tiene 0.77 de precisión, esto significa una precisión del 77% por tanto, es un modelo válido y el ejercicio se realizó correctamente."
      ],
      "metadata": {
        "id": "08y_jG_66uA6"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}